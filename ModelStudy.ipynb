{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a9cb92b-6335-48bc-887e-f9fddc638aab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.4 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\asyncio\\windows_events.py\", line 322, in run_forever\n",
      "    super().run_forever()\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1987, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\alexr\\AppData\\Local\\Temp\\ipykernel_25024\\4025386269.py\", line 1, in <module>\n",
      "    from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments, AutoModel, T5ForConditionalGeneration, T5Tokenizer, DataCollatorForSeq2Seq, AutoModelForCausalLM\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1852, in __getattr__\n",
      "    value = getattr(module, name)\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1851, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1863, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\importlib\\__init__.py\", line 90, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py\", line 21, in <module>\n",
      "    from .auto_factory import (\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 40, in <module>\n",
      "    from ...generation import GenerationMixin\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1851, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1863, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\importlib\\__init__.py\", line 90, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py\", line 53, in <module>\n",
      "    from .candidate_generator import (\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\transformers\\generation\\candidate_generator.py\", line 26, in <module>\n",
      "    from sklearn.metrics import roc_curve\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\sklearn\\__init__.py\", line 87, in <module>\n",
      "    from .base import clone\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 19, in <module>\n",
      "    from .utils import _IS_32BIT\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\__init__.py\", line 16, in <module>\n",
      "    from scipy.sparse import issparse\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\__init__.py\", line 295, in <module>\n",
      "    from ._csr import *\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_csr.py\", line 11, in <module>\n",
      "    from ._sparsetools import (csr_tocsc, csr_tobsr, csr_count_blocks,\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.2.4 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\core\\_multiarray_umath.py:44\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr_name)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# Also print the message (with traceback).  This is because old versions\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# of NumPy unfortunately set up the import to replace (and hide) the\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# error.  The traceback shouldn't be needed, but e.g. pytest plugins\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# seem to swallow it and we should be failing anyway...\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(msg \u001b[38;5;241m+\u001b[39m tb_msg)\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[0;32m     46\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_multiarray_umath, attr_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mImportError\u001b[0m: \nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.2.4 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.auto.modeling_auto because of the following error (look up to see its traceback):\nFailed to import transformers.generation.utils because of the following error (look up to see its traceback):\nnumpy.core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1863\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1862\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   1864\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:995\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:53\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbeam_search\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BeamScorer, BeamSearchScorer, ConstrainedBeamSearchScorer\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcandidate_generator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     54\u001b[0m     AssistedCandidateGenerator,\n\u001b[0;32m     55\u001b[0m     AssistedCandidateGeneratorDifferentTokenizers,\n\u001b[0;32m     56\u001b[0m     CandidateGenerator,\n\u001b[0;32m     57\u001b[0m     EarlyExitCandidateGenerator,\n\u001b[0;32m     58\u001b[0m     PromptLookupCandidateGenerator,\n\u001b[0;32m     59\u001b[0m     _crop_past_key_values,\n\u001b[0;32m     60\u001b[0m     _prepare_attention_mask,\n\u001b[0;32m     61\u001b[0m     _prepare_token_type_ids,\n\u001b[0;32m     62\u001b[0m )\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     64\u001b[0m     NEED_SETUP_CACHE_CLASSES_MAPPING,\n\u001b[0;32m     65\u001b[0m     QUANT_BACKEND_CLASSES_MAPPING,\n\u001b[0;32m     66\u001b[0m     GenerationConfig,\n\u001b[0;32m     67\u001b[0m     GenerationMode,\n\u001b[0;32m     68\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\generation\\candidate_generator.py:26\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sklearn_available():\n\u001b[1;32m---> 26\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m roc_curve\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcache_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DynamicCache\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\__init__.py:87\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     84\u001b[0m     __check_build,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     85\u001b[0m     _distributor_init,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     86\u001b[0m )\n\u001b[1;32m---> 87\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_versions\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:19\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _IS_32BIT\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HTMLDocumentationLinkMixin, estimator_html_repr\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\__init__.py:16\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m issparse\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\__init__.py:295\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m--> 295\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_csr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_csc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_csr.py:11\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _spbase, sparray\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sparsetools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (csr_tocsc, csr_tobsr, csr_count_blocks,\n\u001b[0;32m     12\u001b[0m                            get_csr_submatrix)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sputils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m upcast\n",
      "\u001b[1;31mImportError\u001b[0m: numpy.core.multiarray failed to import",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1863\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1862\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   1864\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:995\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:21\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logging\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto_factory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     22\u001b[0m     _BaseAutoBackboneClass,\n\u001b[0;32m     23\u001b[0m     _BaseAutoModelClass,\n\u001b[0;32m     24\u001b[0m     _LazyAutoMapping,\n\u001b[0;32m     25\u001b[0m     auto_class_update,\n\u001b[0;32m     26\u001b[0m )\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_auto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CONFIG_MAPPING_NAMES\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:40\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GenerationMixin\n\u001b[0;32m     43\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1412\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1851\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1851\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[0;32m   1852\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1865\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1864\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1865\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1866\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1867\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1868\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):\nnumpy.core.multiarray failed to import",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments, AutoModel, T5ForConditionalGeneration, T5Tokenizer, DataCollatorForSeq2Seq, AutoModelForCausalLM\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1412\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1852\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1851\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[1;32m-> 1852\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1853\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n\u001b[0;32m   1854\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1851\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1849\u001b[0m     value \u001b[38;5;241m=\u001b[39m Placeholder\n\u001b[0;32m   1850\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1851\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[0;32m   1852\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1853\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1865\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   1864\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1865\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1866\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1867\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1868\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.models.auto.modeling_auto because of the following error (look up to see its traceback):\nFailed to import transformers.generation.utils because of the following error (look up to see its traceback):\nnumpy.core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments, AutoModel, T5ForConditionalGeneration, T5Tokenizer, DataCollatorForSeq2Seq, AutoModelForCausalLM\n",
    "import torch\n",
    "import json\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from rank_bm25 import BM25Okapi\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import itertools\n",
    "import re\n",
    "import pymorphy2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c04c56c-0fa4-48d1-ba0b-b3488aae8a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2b89689eeba4128b6455e8c46a067ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/39 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27/27 02:38, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.714603</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.722470</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.725352</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 3.7146029472351074, 'eval_accuracy': 0.0, 'eval_f1': 0.0, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_runtime': 0.5939, 'eval_samples_per_second': 6.735, 'eval_steps_per_second': 1.684, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Название модели\n",
    "MODEL_NAME = \"cointegrated/rubert-tiny2\"\n",
    "\n",
    "# Загружаем токенизатор и модель (сначала без указания меток)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Загружаем JSON-датасет\n",
    "dataset = load_dataset(\"json\", data_files=\"computer_faq_extended_dataset.json\")\n",
    "\n",
    "# Создаём словарь меток: каждое уникальное значение ответа получает свой идентификатор\n",
    "unique_answers = list(set([item[\"ответ\"] for item in dataset[\"train\"]]))\n",
    "label2id = {answer: idx for idx, answer in enumerate(unique_answers)}\n",
    "id2label = {idx: answer for answer, idx in label2id.items()}\n",
    "\n",
    "# Загружаем модель с указанием количества классов и сопоставлением меток\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(unique_answers),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# Отправляем модель на GPU, если он доступен\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Токенизация\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"вопрос\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Добавляем числовые метки\n",
    "def add_labels(examples):\n",
    "    examples[\"label\"] = [label2id[ans] for ans in examples[\"ответ\"]]\n",
    "    return examples\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.map(add_labels, batched=True)\n",
    "\n",
    "# Разделяем датасет на обучающую и тестовую выборки\n",
    "train_test_split = tokenized_datasets[\"train\"].train_test_split(test_size=0.1)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "test_dataset = train_test_split[\"test\"]\n",
    "\n",
    "# Загружаем метрики\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "\n",
    "# Функция вычисления метрик\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = torch.argmax(torch.tensor(logits), dim=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
    "        \"f1\": f1_metric.compute(predictions=predictions, references=labels, average=\"weighted\")[\"f1\"],\n",
    "        \"precision\": precision_metric.compute(predictions=predictions, references=labels, average=\"weighted\")[\"precision\"],\n",
    "        \"recall\": recall_metric.compute(predictions=predictions, references=labels, average=\"weighted\")[\"recall\"],\n",
    "    }\n",
    "\n",
    "# Настройки обучения\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./models/rubert_tiny_finetuned\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_steps=500,\n",
    "    logging_dir=\"./logs\",\n",
    "    load_best_model_at_end=True,  # Загружает лучшую модель после обучения\n",
    "    metric_for_best_model=\"f1\",  # Ориентируемся на f1-score\n",
    "    greater_is_better=True,\n",
    ")\n",
    "\n",
    "# Создаём Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Запускаем обучение\n",
    "trainer.train()\n",
    "\n",
    "# Оцениваем модель на тестовой выборке\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\", eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0f1b971-8c7f-4b5b-b7b7-85857b289bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вопрос: Почему компьютер не включается?\n",
      "Ответ: Отключите лишние программы в автозагрузке, проверьте диск на ошибки.\n"
     ]
    }
   ],
   "source": [
    "def answer_question(question):\n",
    "    # Токенизируем вопрос\n",
    "    inputs = tokenizer(question, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Получаем логиты\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    \n",
    "    # Получаем предсказанную метку\n",
    "    predicted_class_id = logits.argmax().item()\n",
    "    # Преобразуем числовую метку обратно в ответ\n",
    "    predicted_answer = id2label[predicted_class_id]\n",
    "    return predicted_answer\n",
    "\n",
    "# Пример вызова функции:\n",
    "question = \"Почему компьютер не включается?\"\n",
    "print(\"Вопрос:\", question)\n",
    "print(\"Ответ:\", answer_question(question))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53d06bf6-21a8-4f69-aa5e-c8d36cd07c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вопрос: Почему компьютер не включается?\n",
      "Найденный ответ: Проверьте кабели, блок питания и исправность розетки.\n",
      "Схожесть: 0.9999999\n"
     ]
    }
   ],
   "source": [
    "# Задаём название модели RuBERT-tiny\n",
    "MODEL_NAME = \"cointegrated/rubert-tiny2\"\n",
    "\n",
    "# Загружаем токенайзер и модель RuBERT-tiny для получения эмбеддингов\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Функция получения эмбеддингов для текста\n",
    "def get_embedding(text):\n",
    "    # Токенизируем текст\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Берём среднее значение по токенам (mean pooling)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    return embeddings.cpu().numpy()[0]\n",
    "\n",
    "# Загружаем датасет FAQ\n",
    "dataset = load_dataset(\"json\", data_files=\"computer_faq_extended_dataset.json\")[\"train\"]\n",
    "\n",
    "# Строим базу: для каждого вопроса сохраняем эмбеддинг и ответ\n",
    "faq_database = []\n",
    "for item in dataset:\n",
    "    question_text = item[\"вопрос\"]\n",
    "    answer_text = item[\"ответ\"]\n",
    "    emb = get_embedding(question_text)\n",
    "    faq_database.append({\n",
    "        \"question\": question_text,\n",
    "        \"answer\": answer_text,\n",
    "        \"embedding\": emb\n",
    "    })\n",
    "\n",
    "# Функция для поиска ближайшего вопроса в базе с помощью косинусного сходства\n",
    "def find_best_answer(user_question):\n",
    "    user_emb = get_embedding(user_question)\n",
    "    # Сравниваем эмбеддинг пользователя с эмбеддингами FAQ\n",
    "    embeddings = np.array([entry[\"embedding\"] for entry in faq_database])\n",
    "    scores = cosine_similarity([user_emb], embeddings)[0]\n",
    "    best_idx = int(np.argmax(scores))\n",
    "    best_match = faq_database[best_idx]\n",
    "    return best_match[\"answer\"], scores[best_idx]\n",
    "\n",
    "# Функция для обработки запроса\n",
    "def answer_question(user_question):\n",
    "    answer, score = find_best_answer(user_question)\n",
    "    return answer, score\n",
    "\n",
    "# Пример использования\n",
    "if __name__ == \"__main__\":\n",
    "    user_question = \"Почему компьютер не включается?\"\n",
    "    answer, score = answer_question(user_question)\n",
    "    print(\"Вопрос:\", user_question)\n",
    "    print(\"Найденный ответ:\", answer)\n",
    "    print(\"Схожесть:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1b38ef6-a878-41fa-9f5f-f4db5be2c367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вопрос: монитор не показывает изображение, что мне делать?\n",
      "Найденный ответ: Проверьте подключение кабелей, исправность монитора и видеокарты.\n",
      "Схожесть: 0.82965595\n"
     ]
    }
   ],
   "source": [
    "# Пример использования\n",
    "if __name__ == \"__main__\":\n",
    "    user_question = \"монитор не показывает изображение, что мне делать?\"\n",
    "    answer, score = answer_question(user_question)\n",
    "    print(\"Вопрос:\", user_question)\n",
    "    print(\"Найденный ответ:\", answer)\n",
    "    print(\"Схожесть:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1eb78795-6efd-4014-a801-267b10652f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вопрос: Почему компьютер не включается?\n",
      "Найденный ответ: Проверьте кабели, блок питания и исправность розетки.\n",
      "Схожесть: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Задаём название модели RuBERT-tiny\n",
    "MODEL_NAME = \"cointegrated/rubert-tiny2\"\n",
    "\n",
    "# Загружаем токенайзер и модель RuBERT-tiny для получения эмбеддингов\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "# Функция получения эмбеддингов для текста\n",
    "def get_embedding(text):\n",
    "    # Токенизируем текст\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Берём среднее значение по токенам (mean pooling)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    return embeddings.cpu().numpy()[0]\n",
    "\n",
    "# Загружаем датасет FAQ\n",
    "dataset = load_dataset(\"json\", data_files=\"computer_faq_extended_dataset.json\")[\"train\"]\n",
    "\n",
    "# Строим базу: для каждого вопроса сохраняем эмбеддинг и ответ\n",
    "faq_database = []\n",
    "for item in dataset:\n",
    "    question_text = item[\"вопрос\"]\n",
    "    answer_text = item[\"ответ\"]\n",
    "    emb = get_embedding(question_text)\n",
    "    faq_database.append({\n",
    "        \"question\": question_text,\n",
    "        \"answer\": answer_text,\n",
    "        \"embedding\": emb\n",
    "    })\n",
    "\n",
    "# Загружаем метрики\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "\n",
    "# Функция вычисления метрик\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = torch.argmax(torch.tensor(logits), dim=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
    "        \"f1\": f1_metric.compute(predictions=predictions, references=labels, average=\"weighted\")[\"f1\"],\n",
    "        \"precision\": precision_metric.compute(predictions=predictions, references=labels, average=\"weighted\")[\"precision\"],\n",
    "        \"recall\": recall_metric.compute(predictions=predictions, references=labels, average=\"weighted\")[\"recall\"],\n",
    "    }\n",
    "\n",
    "# Функция для поиска ближайшего вопроса в базе с помощью косинусного сходства\n",
    "def find_best_answer(user_question):\n",
    "    user_emb = get_embedding(user_question)\n",
    "    # Сравниваем эмбеддинг пользователя с эмбеддингами FAQ\n",
    "    embeddings = np.array([entry[\"embedding\"] for entry in faq_database])\n",
    "    scores = cosine_similarity([user_emb], embeddings)[0]\n",
    "    best_idx = int(np.argmax(scores))\n",
    "    best_match = faq_database[best_idx]\n",
    "    return best_match[\"answer\"], scores[best_idx]\n",
    "\n",
    "# Функция для обработки запроса\n",
    "def answer_question(user_question):\n",
    "    answer, score = find_best_answer(user_question)\n",
    "    return answer, score\n",
    "\n",
    "# Пример использования\n",
    "if __name__ == \"__main__\":\n",
    "    user_question = \"Почему компьютер не включается?\"\n",
    "    answer, score = answer_question(user_question)\n",
    "    print(\"Вопрос:\", user_question)\n",
    "    print(\"Найденный ответ:\", answer)\n",
    "    print(\"Схожесть:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fd688854-a5f1-4340-83bf-f7176d60d92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вопрос: Почему компьютер не включается?\n",
      "Найденный ответ: Проверьте кабели, блок питания и исправность розетки.\n",
      "Схожесть: 1.0\n",
      "Метрики: {'f1': 0.8177777777777778, 'precision': 1.0, 'recall': 0.6917293233082706}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Задаём название модели RuBERT-tiny\n",
    "MODEL_NAME = \"cointegrated/rubert-tiny2\"\n",
    "\n",
    "# Загружаем токенайзер и модель RuBERT-tiny для получения эмбеддингов\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Загружаем метрики\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "\n",
    "# Функция получения эмбеддингов для текста\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    return embeddings.cpu().numpy()[0]\n",
    "\n",
    "# Загружаем датасет FAQ\n",
    "dataset = load_dataset(\"json\", data_files=\"computer_faq_extended_dataset.json\")[\"train\"]\n",
    "\n",
    "# Строим базу: для каждого вопроса сохраняем эмбеддинг и ответ\n",
    "faq_database = []\n",
    "for item in dataset:\n",
    "    question_text = item[\"вопрос\"]\n",
    "    answer_text = item[\"ответ\"]\n",
    "    emb = get_embedding(question_text)\n",
    "    faq_database.append({\n",
    "        \"question\": question_text,\n",
    "        \"answer\": answer_text,\n",
    "        \"embedding\": emb\n",
    "    })\n",
    "\n",
    "# Функция для поиска ближайшего вопроса в базе с помощью косинусного сходства\n",
    "def find_best_answer(user_question, top_n=3, threshold=0.5):\n",
    "    user_emb = get_embedding(user_question)\n",
    "    embeddings = np.array([entry[\"embedding\"] for entry in faq_database])\n",
    "    scores = cosine_similarity([user_emb], embeddings)[0]\n",
    "\n",
    "    # Берём top_n наиболее похожих вопросов\n",
    "    top_indices = np.argsort(scores)[-top_n:][::-1]\n",
    "    top_matches = [(faq_database[i][\"answer\"], scores[i]) for i in top_indices if scores[i] > threshold]\n",
    "\n",
    "    if not top_matches:\n",
    "        return \"Извините, я не смог найти подходящий ответ.\", 0.0\n",
    "\n",
    "    # Выбираем ответ с наибольшим средним баллом или наиболее частый\n",
    "    best_answer = max(top_matches, key=lambda x: x[1])[0]\n",
    "    return best_answer, max(top_matches, key=lambda x: x[1])[1]\n",
    "\n",
    "# Функция для обработки запроса\n",
    "def answer_question(user_question):\n",
    "    answer, score = find_best_answer(user_question)\n",
    "    return answer, score\n",
    "\n",
    "# Функция вычисления метрик\n",
    "def compute_metrics():\n",
    "    similarities = []\n",
    "    for item in dataset:\n",
    "        predicted_answer, _ = find_best_answer(item[\"вопрос\"])\n",
    "        real_emb = get_embedding(item[\"ответ\"])\n",
    "        pred_emb = get_embedding(predicted_answer)\n",
    "        similarity = cosine_similarity([real_emb], [pred_emb])[0][0]\n",
    "        similarities.append(similarity)\n",
    "    return {\n",
    "        \"f1\": f1_metric.compute(predictions=similarities, references=[1.0] * len(similarities), average=\"weighted\")[\"f1\"],\n",
    "        \"precision\": precision_metric.compute(predictions=similarities, references=[1.0] * len(similarities), average=\"weighted\")[\"precision\"],\n",
    "        \"recall\": recall_metric.compute(predictions=similarities, references=[1.0] * len(similarities), average=\"weighted\")[\"recall\"],\n",
    "    }\n",
    "\n",
    "# Пример использования\n",
    "if __name__ == \"__main__\":\n",
    "    user_question = \"Почему компьютер не включается?\"\n",
    "    answer, score = answer_question(user_question)\n",
    "    print(\"Вопрос:\", user_question)\n",
    "    print(\"Найденный ответ:\", answer)\n",
    "    print(\"Схожесть:\", score)\n",
    "    \n",
    "    # Вычисление метрик\n",
    "    metrics = compute_metrics()\n",
    "    print(\"Метрики:\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e59f763f-79b0-41b8-8b71-664107d8337a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вопрос: Я удалил файлы, как восстановить?\n",
      "Найденный ответ: Используйте программы Recuva или встроенные средства восстановления.\n",
      "Схожесть: 0.78735596\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    user_question = \"Я удалил файлы, как восстановить?\"\n",
    "    answer, score = answer_question(user_question)\n",
    "    print(\"Вопрос:\", user_question)\n",
    "    print(\"Найденный ответ:\", answer)\n",
    "    print(\"Схожесть:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "742a052e-e66e-4176-8286-34b590a8bfdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexr\\anaconda3\\envs\\my_rag\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Векторная база знаний успешно создана!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# 1. Загрузка данных\n",
    "with open('computer_faq_extended_dataset.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 2. Инициализация ChromaDB\n",
    "client = chromadb.PersistentClient(path=\"chroma_db\")\n",
    "embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    ")\n",
    "collection = client.get_or_create_collection(\n",
    "    name=\"computer_faq\",\n",
    "    embedding_function=embedding_func,\n",
    ")\n",
    "\n",
    "# 3. Добавление данных в векторную базу\n",
    "collection.add(\n",
    "    documents=[item[\"ответ\"] for item in data],\n",
    "    ids=[str(i) for i in range(len(data))],\n",
    "    metadatas=[{\"question\": item[\"вопрос\"]} for item in data]\n",
    ")\n",
    "\n",
    "# 4. Сохранение модели (в данном случае - только векторная база)\n",
    "print(\"Векторная база знаний успешно создана!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e48348cd-76b8-4d27-9588-09a396d08f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 38 key-value pairs and 291 tensors from models/Llama3-Instruct-8B-RSPO.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Llama 3 Instruct 8B RSPO\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Li11111\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct-RSPO\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Llama-3\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 8B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   9:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv  10:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 128009\n",
      "llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  29:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  30:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  31:                                general.url str              = https://huggingface.co/mradermacher/L...\n",
      "llama_model_loader: - kv  32:              mradermacher.quantize_version str              = 2\n",
      "llama_model_loader: - kv  33:                  mradermacher.quantized_by str              = mradermacher\n",
      "llama_model_loader: - kv  34:                  mradermacher.quantized_at str              = 2025-02-17T21:42:55+01:00\n",
      "llama_model_loader: - kv  35:                  mradermacher.quantized_on str              = rich1\n",
      "llama_model_loader: - kv  36:                         general.source.url str              = https://huggingface.co/li11111/Llama3...\n",
      "llama_model_loader: - kv  37:                  mradermacher.convert_type str              = hf\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 256/128256 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW) \n",
      "llm_load_print_meta: general.name     = Llama 3 Instruct 8B RSPO\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: PAD token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: LF token         = 30 '?'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4685.30 MiB\n",
      "........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    13.02 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   258.50 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'mradermacher.convert_type': 'hf', 'general.name': 'Llama 3 Instruct 8B RSPO', 'general.architecture': 'llama', 'general.type': 'model', 'llama.context_length': '8192', 'general.organization': 'Li11111', 'general.basename': 'Llama-3', 'general.finetune': 'Instruct-RSPO', 'mradermacher.quantized_on': 'rich1', 'general.size_label': '8B', 'general.license': 'apache-2.0', 'llama.block_count': '32', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'general.file_type': '15', 'tokenizer.ggml.eos_token_id': '128009', 'llama.attention.head_count_kv': '8', 'llama.rope.freq_base': '500000.000000', 'mradermacher.quantize_version': '2', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.attention.key_length': '128', 'llama.attention.value_length': '128', 'llama.vocab_size': '128256', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.pre': 'llama-bpe', 'general.source.url': 'https://huggingface.co/li11111/Llama3-Instruct-8B-RSPO', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.ggml.padding_token_id': '128009', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\", 'general.url': 'https://huggingface.co/mradermacher/Llama3-Instruct-8B-RSPO-GGUF', 'mradermacher.quantized_by': 'mradermacher', 'mradermacher.quantized_at': '2025-02-17T21:42:55+01:00'}\n",
      "Using gguf chat template: {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
      "\n",
      "'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}{% endif %}\n",
      "Using chat eos_token: <|eot_id|>\n",
      "Using chat bos_token: <|begin_of_text|>\n",
      "\n",
      "llama_print_timings:        load time =    5610.05 ms\n",
      "llama_print_timings:      sample time =      38.47 ms /    99 runs   (    0.39 ms per token,  2573.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5609.90 ms /    87 tokens (   64.48 ms per token,    15.51 tokens per second)\n",
      "llama_print_timings:        eval time =   15237.70 ms /    98 runs   (  155.49 ms per token,     6.43 tokens per second)\n",
      "llama_print_timings:       total time =   21673.99 ms /   185 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Необходимо проверить физическое состояние питания компьютера и его блоки питания. Если проблема не связана с ними, можно предположить проблему с файловой системой или заражением вирусом. В этом случае попытка использования программы Recuva для восстановления данных или команда `attrib` для проверки и изменений атрибутов файлов может помочь диагностировать и решить проблему с включением компьютера.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    5610.05 ms\n",
      "llama_print_timings:      sample time =      38.34 ms /    99 runs   (    0.39 ms per token,  2582.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3679.21 ms /    59 tokens (   62.36 ms per token,    16.04 tokens per second)\n",
      "llama_print_timings:        eval time =   15254.05 ms /    98 runs   (  155.65 ms per token,     6.42 tokens per second)\n",
      "llama_print_timings:       total time =   19743.96 ms /   157 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Необходимо проверить файловые атрибуты на диске X:, возможно файлы или папки имеют скрытый ('h'), системный ('s') или защищенный ('r') статус, что мешает их отображению на мониторе. Используемая команда 'attrib' или программа Recuva позволяют изменить эти атрибуты и сделать файлы доступными для просмотра на экране.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# Инициализация\n",
    "client = chromadb.PersistentClient(path=\"chroma_db\")\n",
    "embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "collection = client.get_or_create_collection(\"computer_faq\")\n",
    "\n",
    "# Загрузка Llama3\n",
    "llm = Llama(\n",
    "    model_path=\"models/Llama3-Instruct-8B-RSPO.Q4_K_M.gguf\",\n",
    "    n_ctx=2048,\n",
    "    n_threads=4\n",
    ")\n",
    "\n",
    "def get_answer(question):\n",
    "    # Шаг 1: Поиск в датасете\n",
    "    results = collection.query(\n",
    "        query_texts=[question],\n",
    "        n_results=1\n",
    "    )\n",
    "    \n",
    "    exact_answer = results[\"documents\"][0][0] if results[\"documents\"] else None\n",
    "    \n",
    "    # Шаг 2: Проверка схожести вопроса\n",
    "    if results[\"distances\"][0][0] < 0.3:  # Порог схожести\n",
    "        return exact_answer\n",
    "    \n",
    "    # Шаг 3: Генерация ответа для новых вопросов\n",
    "    context = \"\\n\".join(results[\"documents\"][0])\n",
    "    prompt = f\"\"\"Ты IT-специалист. Ответь на вопрос, используя контекст:\n",
    "    \n",
    "    Вопрос: {question}\n",
    "    Контекст: {context}\n",
    "    \n",
    "    Ответ (1-2 предложения):\"\"\"\n",
    "    \n",
    "    output = llm(prompt, max_tokens=150, temperature=0.2)\n",
    "    return output[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "# Примеры\n",
    "print(get_answer(\"Почему компьютер не включается?\"))  # Ответ из датасета\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c54429a8-3c07-45ef-a928-cf2355930fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5610.05 ms\n",
      "llama_print_timings:      sample time =      45.02 ms /   116 runs   (    0.39 ms per token,  2576.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   18229.32 ms /   116 runs   (  157.15 ms per token,     6.36 tokens per second)\n",
      "llama_print_timings:       total time =   19136.80 ms /   117 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Необходимо проверить файловые атрибуты на диске X:, возможно файлы или папки имеют скрытый ('h'), системный ('s') или защищенный ('r') статус, что мешает их отображению на мониторе. Используемая команда 'attrib' или программа Recuva помогут обнаружить и изменить эти атрибуты, если необходимо. Затем нужно повторить попытку просмотра содержимого диска для отображения файлов и изображений на мониторе.\n"
     ]
    }
   ],
   "source": [
    "print(get_answer(\"Почему нет изображения на мониторе?\"))  # Генерация нового ответа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1105aab5-0ec6-426f-b686-c4398ccb17f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexr\\anaconda3\\envs\\my_rag\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "llama_model_loader: loaded meta data with 38 key-value pairs and 291 tensors from models/Llama3-Instruct-8B-RSPO.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Llama 3 Instruct 8B RSPO\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Li11111\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct-RSPO\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Llama-3\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 8B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   9:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv  10:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 128009\n",
      "llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  29:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  30:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  31:                                general.url str              = https://huggingface.co/mradermacher/L...\n",
      "llama_model_loader: - kv  32:              mradermacher.quantize_version str              = 2\n",
      "llama_model_loader: - kv  33:                  mradermacher.quantized_by str              = mradermacher\n",
      "llama_model_loader: - kv  34:                  mradermacher.quantized_at str              = 2025-02-17T21:42:55+01:00\n",
      "llama_model_loader: - kv  35:                  mradermacher.quantized_on str              = rich1\n",
      "llama_model_loader: - kv  36:                         general.source.url str              = https://huggingface.co/li11111/Llama3...\n",
      "llama_model_loader: - kv  37:                  mradermacher.convert_type str              = hf\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 256/128256 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW) \n",
      "llm_load_print_meta: general.name     = Llama 3 Instruct 8B RSPO\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: PAD token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: LF token         = 30 '?'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4685.30 MiB\n",
      "........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    13.02 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   258.50 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'mradermacher.convert_type': 'hf', 'general.name': 'Llama 3 Instruct 8B RSPO', 'general.architecture': 'llama', 'general.type': 'model', 'llama.context_length': '8192', 'general.organization': 'Li11111', 'general.basename': 'Llama-3', 'general.finetune': 'Instruct-RSPO', 'mradermacher.quantized_on': 'rich1', 'general.size_label': '8B', 'general.license': 'apache-2.0', 'llama.block_count': '32', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'general.file_type': '15', 'tokenizer.ggml.eos_token_id': '128009', 'llama.attention.head_count_kv': '8', 'llama.rope.freq_base': '500000.000000', 'mradermacher.quantize_version': '2', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.attention.key_length': '128', 'llama.attention.value_length': '128', 'llama.vocab_size': '128256', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.pre': 'llama-bpe', 'general.source.url': 'https://huggingface.co/li11111/Llama3-Instruct-8B-RSPO', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.ggml.padding_token_id': '128009', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\", 'general.url': 'https://huggingface.co/mradermacher/Llama3-Instruct-8B-RSPO-GGUF', 'mradermacher.quantized_by': 'mradermacher', 'mradermacher.quantized_at': '2025-02-17T21:42:55+01:00'}\n",
      "Using gguf chat template: {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
      "\n",
      "'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}{% endif %}\n",
      "Using chat eos_token: <|eot_id|>\n",
      "Using chat bos_token: <|begin_of_text|>\n",
      "\n",
      "llama_print_timings:        load time =   12542.39 ms\n",
      "llama_print_timings:      sample time =      35.99 ms /    91 runs   (    0.40 ms per token,  2528.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12542.06 ms /   217 tokens (   57.80 ms per token,    17.30 tokens per second)\n",
      "llama_print_timings:        eval time =   14609.96 ms /    90 runs   (  162.33 ms per token,     6.16 tokens per second)\n",
      "llama_print_timings:       total time =   27928.54 ms /   307 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Монитор может не отображать изображение из-за ошибки драйвера видеокарты, проблемы со соединением видеоадаптера или физического повреждения монитора или кабеля VGA/HDMI. Проверьте соответствующие компоненты и обновите драйвер видеокарты, если необходимо. Если проблема persists, обратитесь к технической поддержке или проверьте монитор на повреждения.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   12542.39 ms\n",
      "llama_print_timings:      sample time =      30.04 ms /    75 runs   (    0.40 ms per token,  2497.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9226.39 ms /   167 tokens (   55.25 ms per token,    18.10 tokens per second)\n",
      "llama_print_timings:        eval time =   11909.29 ms /    74 runs   (  160.94 ms per token,     6.21 tokens per second)\n",
      "llama_print_timings:       total time =   21742.13 ms /   241 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Для починки принтера проверьте соединение кабелей, убедитесь в наличии подходящего драйвера и обновите его если необходимости требует. Если проблема persists, попробуйте очистку принтерной головки или обратитесь к инструкции по техническому обслуживанию или контактуруте производителя поддержки.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   12542.39 ms\n",
      "llama_print_timings:      sample time =      40.04 ms /   101 runs   (    0.40 ms per token,  2522.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9952.34 ms /   181 tokens (   54.99 ms per token,    18.19 tokens per second)\n",
      "llama_print_timings:        eval time =   16198.01 ms /   100 runs   (  161.98 ms per token,     6.17 tokens per second)\n",
      "llama_print_timings:       total time =   27009.71 ms /   281 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Если компьютер не включается, проверьте источник питания (заряд батареи или подключение к сети), проверьте блок питания и кабели подключения к монитору и периферийным устройствам. В случае необходимости выполните сброс устройства или обратитесь к инструкции по восстановлению из заводских настроек производителя. Если проблема persists, рекомендуется обратиться к специалисту или контактировать с технической поддержкой производителя.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# Инициализация\n",
    "client = chromadb.PersistentClient(path=\"chroma_db\")\n",
    "embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "collection = client.get_or_create_collection(\"computer_faq\")\n",
    "\n",
    "# Загрузка Llama3\n",
    "llm = Llama(\n",
    "    model_path=\"models/Llama3-Instruct-8B-RSPO.Q4_K_M.gguf\",\n",
    "    n_ctx=2048,\n",
    "    n_threads=4\n",
    ")\n",
    "\n",
    "def get_answer(question):\n",
    "    # Шаг 1: Поиск релевантных ответов\n",
    "    results = collection.query(\n",
    "        query_texts=[question],\n",
    "        n_results=3  # Берем больше результатов для лучшего контекста\n",
    "    )\n",
    "    \n",
    "    # Шаг 2: Проверка релевантности (более строгий порог)\n",
    "    if results[\"distances\"][0][0] < 0.5:  # Более консервативный порог\n",
    "        best_answer = results[\"documents\"][0][0]\n",
    "        best_question = results[\"metadatas\"][0][0][\"question\"]\n",
    "        \n",
    "        # Дополнительная проверка схожести вопроса\n",
    "        if question.lower() in best_question.lower() or best_question.lower() in question.lower():\n",
    "            return best_answer\n",
    "    \n",
    "    # Шаг 3: Подготовка контекста для генерации\n",
    "    context = \"\\n\".join([\n",
    "        f\"Вопрос: {q['question']}\\nОтвет: {a}\" \n",
    "        for q, a in zip(results[\"metadatas\"][0], results[\"documents\"][0])\n",
    "    ])\n",
    "    \n",
    "    # Шаг 4: Генерация ответа с четкими инструкциями\n",
    "    prompt = f\"\"\"Ты опытный IT-специалист. Ответь на вопрос пользователя кратко и по делу.\n",
    "Если вопрос не связан с IT, вежливо сообщи об этом.\n",
    "\n",
    "Вопрос: {question}\n",
    "\n",
    "Релевантные знания из базы:\n",
    "{context}\n",
    "\n",
    "Ответ (1-2 предложения, только по существу):\"\"\"\n",
    "    \n",
    "    output = llm(\n",
    "        prompt,\n",
    "        max_tokens=150,\n",
    "        temperature=0.1,  # Меньше \"креатива\"\n",
    "        stop=[\"\\n\", \"Вопрос:\", \"Контекст:\"]\n",
    "    )\n",
    "    \n",
    "    return output[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "# Примеры\n",
    "print(get_answer(\"Почему нет изображения на мониторе?\"))\n",
    "print(get_answer(\"Как починить принтер?\"))\n",
    "print(get_answer(\"Что делать, если компьютер не включается?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2dfd70e-e8a8-4f3b-a341-8babd1b72606",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12542.39 ms\n",
      "llama_print_timings:      sample time =      25.64 ms /    65 runs   (    0.39 ms per token,  2535.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10022.85 ms /   184 tokens (   54.47 ms per token,    18.36 tokens per second)\n",
      "llama_print_timings:        eval time =   10070.51 ms /    64 runs   (  157.35 ms per token,     6.36 tokens per second)\n",
      "llama_print_timings:       total time =   20617.44 ms /   248 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Файловый менеджер или программка для восстановления файлов (например, Recuva) помогут найти и вернуть пропавшие файлы с флешки. Проверьте подключение флешки и состояние ее файловой системы перед попыткой восстановления данных.\n"
     ]
    }
   ],
   "source": [
    "print(get_answer(\"Микрофон выключился в неожиданный момент\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0d3e28ae-cc32-4a6d-9883-101bd2c533e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 38 key-value pairs and 291 tensors from models/Llama3-Instruct-8B-RSPO.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Llama 3 Instruct 8B RSPO\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Li11111\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct-RSPO\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Llama-3\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 8B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   9:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv  10:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 128009\n",
      "llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  29:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  30:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  31:                                general.url str              = https://huggingface.co/mradermacher/L...\n",
      "llama_model_loader: - kv  32:              mradermacher.quantize_version str              = 2\n",
      "llama_model_loader: - kv  33:                  mradermacher.quantized_by str              = mradermacher\n",
      "llama_model_loader: - kv  34:                  mradermacher.quantized_at str              = 2025-02-17T21:42:55+01:00\n",
      "llama_model_loader: - kv  35:                  mradermacher.quantized_on str              = rich1\n",
      "llama_model_loader: - kv  36:                         general.source.url str              = https://huggingface.co/li11111/Llama3...\n",
      "llama_model_loader: - kv  37:                  mradermacher.convert_type str              = hf\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 256/128256 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW) \n",
      "llm_load_print_meta: general.name     = Llama 3 Instruct 8B RSPO\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: PAD token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: LF token         = 30 '?'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4685.30 MiB\n",
      "........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    17.04 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   288.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'mradermacher.convert_type': 'hf', 'general.name': 'Llama 3 Instruct 8B RSPO', 'general.architecture': 'llama', 'general.type': 'model', 'llama.context_length': '8192', 'general.organization': 'Li11111', 'general.basename': 'Llama-3', 'general.finetune': 'Instruct-RSPO', 'mradermacher.quantized_on': 'rich1', 'general.size_label': '8B', 'general.license': 'apache-2.0', 'llama.block_count': '32', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'general.file_type': '15', 'tokenizer.ggml.eos_token_id': '128009', 'llama.attention.head_count_kv': '8', 'llama.rope.freq_base': '500000.000000', 'mradermacher.quantize_version': '2', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.attention.key_length': '128', 'llama.attention.value_length': '128', 'llama.vocab_size': '128256', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.pre': 'llama-bpe', 'general.source.url': 'https://huggingface.co/li11111/Llama3-Instruct-8B-RSPO', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.ggml.padding_token_id': '128009', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\", 'general.url': 'https://huggingface.co/mradermacher/Llama3-Instruct-8B-RSPO-GGUF', 'mradermacher.quantized_by': 'mradermacher', 'mradermacher.quantized_at': '2025-02-17T21:42:55+01:00'}\n",
      "Using gguf chat template: {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
      "\n",
      "'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}{% endif %}\n",
      "Using chat eos_token: <|eot_id|>\n",
      "Using chat bos_token: <|begin_of_text|>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# Инициализация\n",
    "client = chromadb.PersistentClient(path=\"chroma_db\")\n",
    "embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "collection = client.get_or_create_collection(\"computer_faq\")\n",
    "\n",
    "# Загрузка Llama3 с оптимизированными параметрами\n",
    "llm = Llama(\n",
    "    model_path=\"models/Llama3-Instruct-8B-RSPO.Q4_K_M.gguf\",\n",
    "    n_ctx=4096,  # Увеличенный контекст\n",
    "    n_threads=6,\n",
    "    n_gpu_layers=50  # Если есть GPU\n",
    ")\n",
    "\n",
    "def get_answer(question):\n",
    "    # 1. Поиск релевантного контекста (даже слабые совпадения)\n",
    "    results = collection.query(\n",
    "        query_texts=[question],\n",
    "        n_results=5,\n",
    "        include=[\"documents\", \"metadatas\"]\n",
    "    )\n",
    "    \n",
    "    # 2. Формирование динамического контекста\n",
    "    context = \"Возможные решения из базы знаний:\\n\"\n",
    "    for meta, doc in zip(results[\"metadatas\"][0], results[\"documents\"][0]):\n",
    "        context += f\"- {meta['question']}: {doc}\\n\"\n",
    "    \n",
    "    # 3. Универсальный промпт с \"страховкой\" для любых вопросов\n",
    "    prompt = f\"\"\"Ты опытный IT-специалист. Отвечай на вопросы, используя:\n",
    "1. Контекст из базы знаний (если релевантен)\n",
    "2. Собственные экспертные знания\n",
    "3. Логические выводы\n",
    "\n",
    "Контекст:\n",
    "{context}\n",
    "\n",
    "Вопрос: {question}\n",
    "\n",
    "Структура ответа:\n",
    "1. Основное решение (1-2 предложения)\n",
    "2. Дополнительные проверки (если нужно)\n",
    "3. Альтернативные варианты (если применимо)\n",
    "\n",
    "Ответ:\"\"\"\n",
    "    \n",
    "    # Первичная генерация\n",
    "    output = llm(prompt, max_tokens=500, temperature=0.3)\n",
    "    answer = output[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "    # Принудительное завершение если ответ неполный\n",
    "    if not answer.count('\\n') >= 3:  # Менее 3 пунктов\n",
    "        continuation = llm(\n",
    "            f\"Заверши ответ:\\n{answer}\",\n",
    "            max_tokens=200,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        answer += \"\\n\" + continuation[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b2ebac7-552a-4930-a704-d97aa0ea10f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   18819.44 ms\n",
      "llama_print_timings:      sample time =      46.91 ms /   121 runs   (    0.39 ms per token,  2579.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18819.18 ms /   345 tokens (   54.55 ms per token,    18.33 tokens per second)\n",
      "llama_print_timings:        eval time =   19245.16 ms /   120 runs   (  160.38 ms per token,     6.24 tokens per second)\n",
      "llama_print_timings:       total time =   39050.44 ms /   465 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "При неожиданном выключении микрофона в Windows рекомендуется проверить его свойства в Панели управления звуком (например, через «Сonus» или «Звук» в Панели управления). Перейдите в раздел «Recording devices» или «Микрофоны», найдите выключенный микрофон и убедитесь, что он не отключен или не заблокирован. Если проблема persists, проверьте драйвера звукового адаптера и обновите их, если необходимости требует.\n"
     ]
    }
   ],
   "source": [
    "print(get_answer(\"Микрофон выключился в неожиданный момент\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "279897c9-b5c9-468b-9647-3eeca2272081",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   18428.48 ms\n",
      "llama_print_timings:      sample time =      82.70 ms /   212 runs   (    0.39 ms per token,  2563.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14720.60 ms /   269 tokens (   54.72 ms per token,    18.27 tokens per second)\n",
      "llama_print_timings:        eval time =   33750.27 ms /   211 runs   (  159.95 ms per token,     6.25 tokens per second)\n",
      "llama_print_timings:       total time =   50250.83 ms /   480 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Для настройки RGB-подсветки на нестандартной клавиатуре рекомендуется использовать специализированное программное обеспечение от производителя клавиатуры или третьих разработчиков,such as Keyboard Lighting Software или similar tools.\n",
      "\n",
      "Перед настройкой проверьте,是否 клавиатура поддерживает RGB-подсветку и имеет соответствующие программные инструменты для ее управления. В противном случае может потребоваться поиск альтернативного решения или проверка документации производителя клавиатуры для настройки подсветки.\n",
      "\n",
      "В качестве альтернативы можно попробовать использовать всеобщие инструменты для управления освещением, такие как RGB-control software или даже некоторые медиаплэеры с поддержкой освещения для клавиатур (если они совместимы с вашей клавиатурой). Однако эффективность таких решений может зависеть от конкретной модели клавиатуры и ее программного обеспечения.\n"
     ]
    }
   ],
   "source": [
    "print(get_answer(\"Как настроить RGB-подсветку на нестандартной клавиатуре?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ac9e1c2-b675-4ed8-889c-59ac24ee5f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   18819.44 ms\n",
      "llama_print_timings:      sample time =      20.18 ms /    52 runs   (    0.39 ms per token,  2576.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9573.38 ms /   172 tokens (   55.66 ms per token,    17.97 tokens per second)\n",
      "llama_print_timings:        eval time =    8042.15 ms /    51 runs   (  157.69 ms per token,     6.34 tokens per second)\n",
      "llama_print_timings:       total time =   18028.05 ms /   223 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "При работе беспроводной мыши на стеклянном столе может возникать дергание из-за слабого сигнала радиосигнала к приемнику мыши или помехи от отражения радиоволн от поверхности стола.\n"
     ]
    }
   ],
   "source": [
    "print(get_answer(\"Почему моя беспроводная мышь дергается при работе на стеклянном столе?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "acdfbef8-20a1-4491-9bc2-bd0e8c604bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   18819.44 ms\n",
      "llama_print_timings:      sample time =      26.26 ms /    69 runs   (    0.38 ms per token,  2627.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14104.98 ms /   256 tokens (   55.10 ms per token,    18.15 tokens per second)\n",
      "llama_print_timings:        eval time =   10734.17 ms /    68 runs   (  157.86 ms per token,     6.33 tokens per second)\n",
      "llama_print_timings:       total time =   25399.79 ms /   324 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Для переноса Windows с HDD на NVMe SSD без переустановки рекомендуется использовать инструмент «Disk Clone» или «Disk Imaging» программы Acronis True Image или EaseUS Todo Backup. Эти программы позволяют создать точную копию жесткого диска HDD и записать ее на новый SSD NVMe.\n"
     ]
    }
   ],
   "source": [
    "print(get_answer(\"Как перенести Windows с HDD на NVMe SSD без переустановки?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6511da76-fb7f-4ade-a9f7-ab2b2298afdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   18819.44 ms\n",
      "llama_print_timings:      sample time =      19.50 ms /    51 runs   (    0.38 ms per token,  2615.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12404.23 ms /   225 tokens (   55.13 ms per token,    18.14 tokens per second)\n",
      "llama_print_timings:        eval time =    7906.86 ms /    50 runs   (  158.14 ms per token,     6.32 tokens per second)\n",
      "llama_print_timings:       total time =   20717.98 ms /   275 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "При появлении размытого изображения лица в Zoom при использовании виртуального фона возможной причиной является низкая разрешающая способность видеокамеры или неоптимальная настройка камеры в системе.\n"
     ]
    }
   ],
   "source": [
    "print(get_answer(\"Почему в Zoom мое лицо выглядит размытым при виртуальном фоне?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c0bfb7c6-9fdc-4ddd-b3f3-f69e5ced17e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   18819.44 ms\n",
      "llama_print_timings:      sample time =      27.36 ms /    70 runs   (    0.39 ms per token,  2558.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =   13191.23 ms /   238 tokens (   55.43 ms per token,    18.04 tokens per second)\n",
      "llama_print_timings:        eval time =   10915.43 ms /    69 runs   (  158.19 ms per token,     6.32 tokens per second)\n",
      "llama_print_timings:       total time =   24676.71 ms /   307 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "При заряжении ноутбука только в одном определенном положении кабеля может возникнуть проблема с контактами зажимов или повреждением гибкого кабеля. Основное решение consists in проверке и очистке контактов зажимов от пыли и грязи или заменой поврежденного кабеля.\n"
     ]
    }
   ],
   "source": [
    "print(get_answer(\"Что делать, если ноутбук заряжается только в одном определенном положении кабеля?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4059952c-2ade-45eb-85fc-4c774f245307",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   18819.44 ms\n",
      "llama_print_timings:      sample time =      17.65 ms /    46 runs   (    0.38 ms per token,  2605.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4730.77 ms /    80 tokens (   59.13 ms per token,    16.91 tokens per second)\n",
      "llama_print_timings:        eval time =    7268.62 ms /    45 runs   (  161.52 ms per token,     6.19 tokens per second)\n",
      "llama_print_timings:       total time =   12380.63 ms /   125 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Для настройки цветопередачи монитора для профессионального фоторедактирования рекомендуется изменить параметры цветового профиля в настройках монитора или видеокарты.\n"
     ]
    }
   ],
   "source": [
    "print(get_answer(\"Как настроить цветопередачу монитора для профессионального фоторедактирования?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "24fd4135-4361-44fc-a3d5-d6192dcfdd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   18428.48 ms\n",
      "llama_print_timings:      sample time =     106.87 ms /   272 runs   (    0.39 ms per token,  2545.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15100.96 ms /   280 tokens (   53.93 ms per token,    18.54 tokens per second)\n",
      "llama_print_timings:        eval time =   43126.36 ms /   271 runs   (  159.14 ms per token,     6.28 tokens per second)\n",
      "llama_print_timings:       total time =   60500.39 ms /   551 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "При подключении Bluetooth-наушников к телефону они устанавливают парковку с устройством и обмениваются информацией о подключении. Однако в Windows может возникнуть проблема с обнаружением устройств Bluetooth из-за следующих причин:\n",
      "\n",
      "Основное решение: Проверьте панель устройств Bluetooth в Windows («Устройства» → «Бluetooth» или через центр управления Bluetooth в Панели управления). Убедитесь, что Bluetooth-сервис активен и устройство наушников отображается в списке доступных устройств.\n",
      "\n",
      "Дополнительные проверки: Обратите внимание на уровень сигнала Bluetooth-сигнала в наушниках и на компьютере. Если сигнал слабый, попытайтесь переместить устройство ближе к компьютеру или изменить позицию антенны в наушниках.\n",
      "\n",
      "Альтернативные варианты: Если проблема persists, можно попытаться обновить драйвера Bluetooth в системе или деинсталлировать и再инсталлировать устройство наушников в Windows Device Manager. В крайнем случае, обратитесь к производителю наушников за поддержкой или обновлениями драйверов специфических для вашего устройства.\n"
     ]
    }
   ],
   "source": [
    "print(get_answer(\"Почему Bluetooth-наушники подключаются к телефону, но не видят Windows?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a501f6ec-0210-4b78-a386-d5b349334393",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   18428.48 ms\n",
      "llama_print_timings:      sample time =     106.79 ms /   276 runs   (    0.39 ms per token,  2584.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18428.08 ms /   339 tokens (   54.36 ms per token,    18.40 tokens per second)\n",
      "llama_print_timings:        eval time =   43793.24 ms /   275 runs   (  159.25 ms per token,     6.28 tokens per second)\n",
      "llama_print_timings:       total time =   64512.53 ms /   614 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Для раздачи интернета с ПК через Ethernet-кабель на Smart TV необходимо:\n",
      "\n",
      "Основное решение: подключить Ethernet-кабель от компьютера к Smart TV и настроить сетевой шлюз или DHCP-сервер на ПК для обеспечения доступа к интернету на телевизоре.\n",
      "\n",
      "Дополнительные проверки: убедитесь, что Ethernet-кабель правильно подключен к обоим устройствам (ПК и Smart TV), а также что сетевая карта на ПК и Smart TV поддерживают соответствующие протоколы и стандарты (например, DHCP и IPv4 или IPv6).\n",
      "\n",
      "Альтернативные варианты: если у вас доступен WiFi на ПК и Smart TV, можно использовать беспроводной способ подключения к интернету и SKIP Ethernet-кабель. В этом случае настройте WiFi-сеть на ПК и Smart TV соответственно. Если у вас нет доступа к настройкам Smart TV или возникнут проблемы с Ethernet-соединением, можно использовать дополнительное сетевое устройство,such as a wireless router or a network switch, для обеспечения доступа к интернету от ПК к Smart TV. Однако это может потребовать дополнительных настроек и конфигураций.\n"
     ]
    }
   ],
   "source": [
    "print(get_answer(\"Как раздать интернет с ПК через Ethernet-кабель на Smart TV?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e5d858bc-5252-4040-b093-82b11aa3ee98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   18819.44 ms\n",
      "llama_print_timings:      sample time =      25.33 ms /    65 runs   (    0.39 ms per token,  2566.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7834.99 ms /   141 tokens (   55.57 ms per token,    18.00 tokens per second)\n",
      "llama_print_timings:        eval time =   10178.93 ms /    64 runs   (  159.05 ms per token,     6.29 tokens per second)\n",
      "llama_print_timings:       total time =   18540.40 ms /   205 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "При сбросе настроек разгона после обновления BIOS возможной причиной является изменение конфигурации утилитами фирмы-производителя или автоматическим сбросом заводских значений при обновлении биоса для обеспечения стабильной работы системы.\n"
     ]
    }
   ],
   "source": [
    "print(get_answer(\"Почему после обновления BIOS сбросились настройки разгона?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "18dc0a83-f19e-418e-864b-3c58e3aca4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   18819.44 ms\n",
      "llama_print_timings:      sample time =      18.98 ms /    48 runs   (    0.40 ms per token,  2529.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14657.76 ms /   268 tokens (   54.69 ms per token,    18.28 tokens per second)\n",
      "llama_print_timings:        eval time =    7429.70 ms /    47 runs   (  158.08 ms per token,     6.33 tokens per second)\n",
      "llama_print_timings:       total time =   22476.44 ms /   315 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Для предотвращения отключения второго монитора при сворачивании игр рекомендуется использовать функцию «Постоянное отображение экрана» в настройках видеокарты или монитора.\n"
     ]
    }
   ],
   "source": [
    "print(get_answer(\"Как сделать так, чтобы второй монитор не отключался при сворачивании игр?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "98c148d2-65c5-40c3-a205-2c5bf41cb16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   19182.00 ms\n",
      "llama_print_timings:      sample time =      89.75 ms /   229 runs   (    0.39 ms per token,  2551.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =   19181.71 ms /   355 tokens (   54.03 ms per token,    18.51 tokens per second)\n",
      "llama_print_timings:        eval time =   36316.63 ms /   228 runs   (  159.28 ms per token,     6.28 tokens per second)\n",
      "llama_print_timings:       total time =   57532.46 ms /   583 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "При отсутствии зарядки ноутбука от Thunderbolt-док-станции, но при нормальной работе других устройств, вероятно, проблема связана с неисправностью зарядного модуля или несовместимостью станции с конкретным типом ноутбука или его зарядной системы.\n",
      "\n",
      "Дополнительная проверка: Проверьте соответствие станции и ноутбука по следующим параметрам:\n",
      "* Тип зарядки (USB-C Power Delivery или Thunderbolt Power Delivery);\n",
      "* Максимальная мощность заряда;\n",
      "* Поддерживаемые протоколы зарядки.\n",
      "\n",
      "Альтернативный вариант: Возможно, проблема вызвана настройкой станции или конфликтом с другими устройствами, подключенными к ней. Попробуйте отключить другие устройства и проверить зарядку ноутбука отдельно. Если проблема persists, рекомендуется обратиться к производителю станции или контактировать с технической поддержкой для диагностического обследования и решения проблемы.\n"
     ]
    }
   ],
   "source": [
    "print(get_answer(\"Почему Thunderbolt-док-станция не заряжает ноутбук, но устройства работают?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbcda68-34bb-46d2-a975-dfc12a8e5b0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
