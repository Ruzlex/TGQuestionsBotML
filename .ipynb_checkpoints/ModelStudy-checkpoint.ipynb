{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a9cb92b-6335-48bc-887e-f9fddc638aab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.4 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\asyncio\\windows_events.py\", line 322, in run_forever\n",
      "    super().run_forever()\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1987, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\alexr\\AppData\\Local\\Temp\\ipykernel_25024\\4025386269.py\", line 1, in <module>\n",
      "    from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments, AutoModel, T5ForConditionalGeneration, T5Tokenizer, DataCollatorForSeq2Seq, AutoModelForCausalLM\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1852, in __getattr__\n",
      "    value = getattr(module, name)\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1851, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1863, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\importlib\\__init__.py\", line 90, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py\", line 21, in <module>\n",
      "    from .auto_factory import (\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 40, in <module>\n",
      "    from ...generation import GenerationMixin\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1851, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1863, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\importlib\\__init__.py\", line 90, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py\", line 53, in <module>\n",
      "    from .candidate_generator import (\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\transformers\\generation\\candidate_generator.py\", line 26, in <module>\n",
      "    from sklearn.metrics import roc_curve\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\sklearn\\__init__.py\", line 87, in <module>\n",
      "    from .base import clone\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 19, in <module>\n",
      "    from .utils import _IS_32BIT\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\__init__.py\", line 16, in <module>\n",
      "    from scipy.sparse import issparse\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\__init__.py\", line 295, in <module>\n",
      "    from ._csr import *\n",
      "  File \"C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_csr.py\", line 11, in <module>\n",
      "    from ._sparsetools import (csr_tocsc, csr_tobsr, csr_count_blocks,\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.2.4 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\core\\_multiarray_umath.py:44\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr_name)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# Also print the message (with traceback).  This is because old versions\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# of NumPy unfortunately set up the import to replace (and hide) the\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# error.  The traceback shouldn't be needed, but e.g. pytest plugins\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# seem to swallow it and we should be failing anyway...\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(msg \u001b[38;5;241m+\u001b[39m tb_msg)\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[0;32m     46\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_multiarray_umath, attr_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mImportError\u001b[0m: \nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.2.4 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.auto.modeling_auto because of the following error (look up to see its traceback):\nFailed to import transformers.generation.utils because of the following error (look up to see its traceback):\nnumpy.core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1863\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1862\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   1864\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:995\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:53\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbeam_search\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BeamScorer, BeamSearchScorer, ConstrainedBeamSearchScorer\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcandidate_generator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     54\u001b[0m     AssistedCandidateGenerator,\n\u001b[0;32m     55\u001b[0m     AssistedCandidateGeneratorDifferentTokenizers,\n\u001b[0;32m     56\u001b[0m     CandidateGenerator,\n\u001b[0;32m     57\u001b[0m     EarlyExitCandidateGenerator,\n\u001b[0;32m     58\u001b[0m     PromptLookupCandidateGenerator,\n\u001b[0;32m     59\u001b[0m     _crop_past_key_values,\n\u001b[0;32m     60\u001b[0m     _prepare_attention_mask,\n\u001b[0;32m     61\u001b[0m     _prepare_token_type_ids,\n\u001b[0;32m     62\u001b[0m )\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     64\u001b[0m     NEED_SETUP_CACHE_CLASSES_MAPPING,\n\u001b[0;32m     65\u001b[0m     QUANT_BACKEND_CLASSES_MAPPING,\n\u001b[0;32m     66\u001b[0m     GenerationConfig,\n\u001b[0;32m     67\u001b[0m     GenerationMode,\n\u001b[0;32m     68\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\generation\\candidate_generator.py:26\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sklearn_available():\n\u001b[1;32m---> 26\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m roc_curve\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcache_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DynamicCache\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\__init__.py:87\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     84\u001b[0m     __check_build,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     85\u001b[0m     _distributor_init,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     86\u001b[0m )\n\u001b[1;32m---> 87\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_versions\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:19\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _IS_32BIT\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HTMLDocumentationLinkMixin, estimator_html_repr\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\__init__.py:16\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m issparse\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\__init__.py:295\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m--> 295\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_csr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_csc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_csr.py:11\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _spbase, sparray\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sparsetools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (csr_tocsc, csr_tobsr, csr_count_blocks,\n\u001b[0;32m     12\u001b[0m                            get_csr_submatrix)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sputils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m upcast\n",
      "\u001b[1;31mImportError\u001b[0m: numpy.core.multiarray failed to import",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1863\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1862\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   1864\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:995\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:21\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logging\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto_factory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     22\u001b[0m     _BaseAutoBackboneClass,\n\u001b[0;32m     23\u001b[0m     _BaseAutoModelClass,\n\u001b[0;32m     24\u001b[0m     _LazyAutoMapping,\n\u001b[0;32m     25\u001b[0m     auto_class_update,\n\u001b[0;32m     26\u001b[0m )\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_auto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CONFIG_MAPPING_NAMES\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:40\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GenerationMixin\n\u001b[0;32m     43\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1412\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1851\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1851\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[0;32m   1852\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1865\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1864\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1865\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1866\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1867\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1868\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):\nnumpy.core.multiarray failed to import",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments, AutoModel, T5ForConditionalGeneration, T5Tokenizer, DataCollatorForSeq2Seq, AutoModelForCausalLM\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1412\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1852\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1851\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[1;32m-> 1852\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1853\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n\u001b[0;32m   1854\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1851\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1849\u001b[0m     value \u001b[38;5;241m=\u001b[39m Placeholder\n\u001b[0;32m   1850\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1851\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[0;32m   1852\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1853\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1865\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   1864\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1865\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1866\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1867\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1868\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.models.auto.modeling_auto because of the following error (look up to see its traceback):\nFailed to import transformers.generation.utils because of the following error (look up to see its traceback):\nnumpy.core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments, AutoModel, T5ForConditionalGeneration, T5Tokenizer, DataCollatorForSeq2Seq, AutoModelForCausalLM\n",
    "import torch\n",
    "import json\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from rank_bm25 import BM25Okapi\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import itertools\n",
    "import re\n",
    "import pymorphy2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c04c56c-0fa4-48d1-ba0b-b3488aae8a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2b89689eeba4128b6455e8c46a067ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/39 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27/27 02:38, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.714603</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.722470</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.725352</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 3.7146029472351074, 'eval_accuracy': 0.0, 'eval_f1': 0.0, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_runtime': 0.5939, 'eval_samples_per_second': 6.735, 'eval_steps_per_second': 1.684, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "MODEL_NAME = \"cointegrated/rubert-tiny2\"\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ –º–æ–¥–µ–ª—å (—Å–Ω–∞—á–∞–ª–∞ –±–µ–∑ —É–∫–∞–∑–∞–Ω–∏—è –º–µ—Ç–æ–∫)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º JSON-–¥–∞—Ç–∞—Å–µ—Ç\n",
    "dataset = load_dataset(\"json\", data_files=\"computer_faq_extended_dataset.json\")\n",
    "\n",
    "# –°–æ–∑–¥–∞—ë–º —Å–ª–æ–≤–∞—Ä—å –º–µ—Ç–æ–∫: –∫–∞–∂–¥–æ–µ —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –ø–æ–ª—É—á–∞–µ—Ç —Å–≤–æ–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä\n",
    "unique_answers = list(set([item[\"–æ—Ç–≤–µ—Ç\"] for item in dataset[\"train\"]]))\n",
    "label2id = {answer: idx for idx, answer in enumerate(unique_answers)}\n",
    "id2label = {idx: answer for answer, idx in label2id.items()}\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Å–æ–≤ –∏ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ–º –º–µ—Ç–æ–∫\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(unique_answers),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –º–æ–¥–µ–ª—å –Ω–∞ GPU, –µ—Å–ª–∏ –æ–Ω –¥–æ—Å—Ç—É–ø–µ–Ω\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"–≤–æ–ø—Ä–æ—Å\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# –î–æ–±–∞–≤–ª—è–µ–º —á–∏—Å–ª–æ–≤—ã–µ –º–µ—Ç–∫–∏\n",
    "def add_labels(examples):\n",
    "    examples[\"label\"] = [label2id[ans] for ans in examples[\"–æ—Ç–≤–µ—Ç\"]]\n",
    "    return examples\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.map(add_labels, batched=True)\n",
    "\n",
    "# –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞—Ç–∞—Å–µ—Ç –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏\n",
    "train_test_split = tokenized_datasets[\"train\"].train_test_split(test_size=0.1)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "test_dataset = train_test_split[\"test\"]\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = torch.argmax(torch.tensor(logits), dim=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
    "        \"f1\": f1_metric.compute(predictions=predictions, references=labels, average=\"weighted\")[\"f1\"],\n",
    "        \"precision\": precision_metric.compute(predictions=predictions, references=labels, average=\"weighted\")[\"precision\"],\n",
    "        \"recall\": recall_metric.compute(predictions=predictions, references=labels, average=\"weighted\")[\"recall\"],\n",
    "    }\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—É—á–µ–Ω–∏—è\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./models/rubert_tiny_finetuned\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_steps=500,\n",
    "    logging_dir=\"./logs\",\n",
    "    load_best_model_at_end=True,  # –ó–∞–≥—Ä—É–∂–∞–µ—Ç –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è\n",
    "    metric_for_best_model=\"f1\",  # –û—Ä–∏–µ–Ω—Ç–∏—Ä—É–µ–º—Å—è –Ω–∞ f1-score\n",
    "    greater_is_better=True,\n",
    ")\n",
    "\n",
    "# –°–æ–∑–¥–∞—ë–º Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ\n",
    "trainer.train()\n",
    "\n",
    "# –û—Ü–µ–Ω–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\", eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0f1b971-8c7f-4b5b-b7b7-85857b289bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–í–æ–ø—Ä–æ—Å: –ü–æ—á–µ–º—É –∫–æ–º–ø—å—é—Ç–µ—Ä –Ω–µ –≤–∫–ª—é—á–∞–µ—Ç—Å—è?\n",
      "–û—Ç–≤–µ—Ç: –û—Ç–∫–ª—é—á–∏—Ç–µ –ª–∏—à–Ω–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã –≤ –∞–≤—Ç–æ–∑–∞–≥—Ä—É–∑–∫–µ, –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–∏—Å–∫ –Ω–∞ –æ—à–∏–±–∫–∏.\n"
     ]
    }
   ],
   "source": [
    "def answer_question(question):\n",
    "    # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –≤–æ–ø—Ä–æ—Å\n",
    "    inputs = tokenizer(question, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # –ü–æ–ª—É—á–∞–µ–º –ª–æ–≥–∏—Ç—ã\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    \n",
    "    # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—É—é –º–µ—Ç–∫—É\n",
    "    predicted_class_id = logits.argmax().item()\n",
    "    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —á–∏—Å–ª–æ–≤—É—é –º–µ—Ç–∫—É –æ–±—Ä–∞—Ç–Ω–æ –≤ –æ—Ç–≤–µ—Ç\n",
    "    predicted_answer = id2label[predicted_class_id]\n",
    "    return predicted_answer\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä –≤—ã–∑–æ–≤–∞ —Ñ—É–Ω–∫—Ü–∏–∏:\n",
    "question = \"–ü–æ—á–µ–º—É –∫–æ–º–ø—å—é—Ç–µ—Ä –Ω–µ –≤–∫–ª—é—á–∞–µ—Ç—Å—è?\"\n",
    "print(\"–í–æ–ø—Ä–æ—Å:\", question)\n",
    "print(\"–û—Ç–≤–µ—Ç:\", answer_question(question))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53d06bf6-21a8-4f69-aa5e-c8d36cd07c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–í–æ–ø—Ä–æ—Å: –ü–æ—á–µ–º—É –∫–æ–º–ø—å—é—Ç–µ—Ä –Ω–µ –≤–∫–ª—é—á–∞–µ—Ç—Å—è?\n",
      "–ù–∞–π–¥–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç: –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–∞–±–µ–ª–∏, –±–ª–æ–∫ –ø–∏—Ç–∞–Ω–∏—è –∏ –∏—Å–ø—Ä–∞–≤–Ω–æ—Å—Ç—å —Ä–æ–∑–µ—Ç–∫–∏.\n",
      "–°—Ö–æ–∂–µ—Å—Ç—å: 0.9999999\n"
     ]
    }
   ],
   "source": [
    "# –ó–∞–¥–∞—ë–º –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ RuBERT-tiny\n",
    "MODEL_NAME = \"cointegrated/rubert-tiny2\"\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä –∏ –º–æ–¥–µ–ª—å RuBERT-tiny –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —Ç–µ–∫—Å—Ç–∞\n",
    "def get_embedding(text):\n",
    "    # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # –ë–µ—Ä—ë–º —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —Ç–æ–∫–µ–Ω–∞–º (mean pooling)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    return embeddings.cpu().numpy()[0]\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç FAQ\n",
    "dataset = load_dataset(\"json\", data_files=\"computer_faq_extended_dataset.json\")[\"train\"]\n",
    "\n",
    "# –°—Ç—Ä–æ–∏–º –±–∞–∑—É: –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∏ –æ—Ç–≤–µ—Ç\n",
    "faq_database = []\n",
    "for item in dataset:\n",
    "    question_text = item[\"–≤–æ–ø—Ä–æ—Å\"]\n",
    "    answer_text = item[\"–æ—Ç–≤–µ—Ç\"]\n",
    "    emb = get_embedding(question_text)\n",
    "    faq_database.append({\n",
    "        \"question\": question_text,\n",
    "        \"answer\": answer_text,\n",
    "        \"embedding\": emb\n",
    "    })\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞ –±–ª–∏–∂–∞–π—à–µ–≥–æ –≤–æ–ø—Ä–æ—Å–∞ –≤ –±–∞–∑–µ —Å –ø–æ–º–æ—â—å—é –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞\n",
    "def find_best_answer(user_question):\n",
    "    user_emb = get_embedding(user_question)\n",
    "    # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏ FAQ\n",
    "    embeddings = np.array([entry[\"embedding\"] for entry in faq_database])\n",
    "    scores = cosine_similarity([user_emb], embeddings)[0]\n",
    "    best_idx = int(np.argmax(scores))\n",
    "    best_match = faq_database[best_idx]\n",
    "    return best_match[\"answer\"], scores[best_idx]\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞\n",
    "def answer_question(user_question):\n",
    "    answer, score = find_best_answer(user_question)\n",
    "    return answer, score\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
    "if __name__ == \"__main__\":\n",
    "    user_question = \"–ü–æ—á–µ–º—É –∫–æ–º–ø—å—é—Ç–µ—Ä –Ω–µ –≤–∫–ª—é—á–∞–µ—Ç—Å—è?\"\n",
    "    answer, score = answer_question(user_question)\n",
    "    print(\"–í–æ–ø—Ä–æ—Å:\", user_question)\n",
    "    print(\"–ù–∞–π–¥–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç:\", answer)\n",
    "    print(\"–°—Ö–æ–∂–µ—Å—Ç—å:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1b38ef6-a878-41fa-9f5f-f4db5be2c367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–í–æ–ø—Ä–æ—Å: –º–æ–Ω–∏—Ç–æ—Ä –Ω–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, —á—Ç–æ –º–Ω–µ –¥–µ–ª–∞—Ç—å?\n",
      "–ù–∞–π–¥–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç: –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫–∞–±–µ–ª–µ–π, –∏—Å–ø—Ä–∞–≤–Ω–æ—Å—Ç—å –º–æ–Ω–∏—Ç–æ—Ä–∞ –∏ –≤–∏–¥–µ–æ–∫–∞—Ä—Ç—ã.\n",
      "–°—Ö–æ–∂–µ—Å—Ç—å: 0.82965595\n"
     ]
    }
   ],
   "source": [
    "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
    "if __name__ == \"__main__\":\n",
    "    user_question = \"–º–æ–Ω–∏—Ç–æ—Ä –Ω–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, —á—Ç–æ –º–Ω–µ –¥–µ–ª–∞—Ç—å?\"\n",
    "    answer, score = answer_question(user_question)\n",
    "    print(\"–í–æ–ø—Ä–æ—Å:\", user_question)\n",
    "    print(\"–ù–∞–π–¥–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç:\", answer)\n",
    "    print(\"–°—Ö–æ–∂–µ—Å—Ç—å:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1eb78795-6efd-4014-a801-267b10652f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–í–æ–ø—Ä–æ—Å: –ü–æ—á–µ–º—É –∫–æ–º–ø—å—é—Ç–µ—Ä –Ω–µ –≤–∫–ª—é—á–∞–µ—Ç—Å—è?\n",
      "–ù–∞–π–¥–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç: –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–∞–±–µ–ª–∏, –±–ª–æ–∫ –ø–∏—Ç–∞–Ω–∏—è –∏ –∏—Å–ø—Ä–∞–≤–Ω–æ—Å—Ç—å —Ä–æ–∑–µ—Ç–∫–∏.\n",
      "–°—Ö–æ–∂–µ—Å—Ç—å: 1.0\n"
     ]
    }
   ],
   "source": [
    "# –ó–∞–¥–∞—ë–º –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ RuBERT-tiny\n",
    "MODEL_NAME = \"cointegrated/rubert-tiny2\"\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä –∏ –º–æ–¥–µ–ª—å RuBERT-tiny –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —Ç–µ–∫—Å—Ç–∞\n",
    "def get_embedding(text):\n",
    "    # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # –ë–µ—Ä—ë–º —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —Ç–æ–∫–µ–Ω–∞–º (mean pooling)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    return embeddings.cpu().numpy()[0]\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç FAQ\n",
    "dataset = load_dataset(\"json\", data_files=\"computer_faq_extended_dataset.json\")[\"train\"]\n",
    "\n",
    "# –°—Ç—Ä–æ–∏–º –±–∞–∑—É: –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∏ –æ—Ç–≤–µ—Ç\n",
    "faq_database = []\n",
    "for item in dataset:\n",
    "    question_text = item[\"–≤–æ–ø—Ä–æ—Å\"]\n",
    "    answer_text = item[\"–æ—Ç–≤–µ—Ç\"]\n",
    "    emb = get_embedding(question_text)\n",
    "    faq_database.append({\n",
    "        \"question\": question_text,\n",
    "        \"answer\": answer_text,\n",
    "        \"embedding\": emb\n",
    "    })\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = torch.argmax(torch.tensor(logits), dim=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
    "        \"f1\": f1_metric.compute(predictions=predictions, references=labels, average=\"weighted\")[\"f1\"],\n",
    "        \"precision\": precision_metric.compute(predictions=predictions, references=labels, average=\"weighted\")[\"precision\"],\n",
    "        \"recall\": recall_metric.compute(predictions=predictions, references=labels, average=\"weighted\")[\"recall\"],\n",
    "    }\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞ –±–ª–∏–∂–∞–π—à–µ–≥–æ –≤–æ–ø—Ä–æ—Å–∞ –≤ –±–∞–∑–µ —Å –ø–æ–º–æ—â—å—é –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞\n",
    "def find_best_answer(user_question):\n",
    "    user_emb = get_embedding(user_question)\n",
    "    # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏ FAQ\n",
    "    embeddings = np.array([entry[\"embedding\"] for entry in faq_database])\n",
    "    scores = cosine_similarity([user_emb], embeddings)[0]\n",
    "    best_idx = int(np.argmax(scores))\n",
    "    best_match = faq_database[best_idx]\n",
    "    return best_match[\"answer\"], scores[best_idx]\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞\n",
    "def answer_question(user_question):\n",
    "    answer, score = find_best_answer(user_question)\n",
    "    return answer, score\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
    "if __name__ == \"__main__\":\n",
    "    user_question = \"–ü–æ—á–µ–º—É –∫–æ–º–ø—å—é—Ç–µ—Ä –Ω–µ –≤–∫–ª—é—á–∞–µ—Ç—Å—è?\"\n",
    "    answer, score = answer_question(user_question)\n",
    "    print(\"–í–æ–ø—Ä–æ—Å:\", user_question)\n",
    "    print(\"–ù–∞–π–¥–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç:\", answer)\n",
    "    print(\"–°—Ö–æ–∂–µ—Å—Ç—å:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fd688854-a5f1-4340-83bf-f7176d60d92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–í–æ–ø—Ä–æ—Å: –ü–æ—á–µ–º—É –∫–æ–º–ø—å—é—Ç–µ—Ä –Ω–µ –≤–∫–ª—é—á–∞–µ—Ç—Å—è?\n",
      "–ù–∞–π–¥–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç: –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–∞–±–µ–ª–∏, –±–ª–æ–∫ –ø–∏—Ç–∞–Ω–∏—è –∏ –∏—Å–ø—Ä–∞–≤–Ω–æ—Å—Ç—å —Ä–æ–∑–µ—Ç–∫–∏.\n",
      "–°—Ö–æ–∂–µ—Å—Ç—å: 1.0\n",
      "–ú–µ—Ç—Ä–∏–∫–∏: {'f1': 0.8177777777777778, 'precision': 1.0, 'recall': 0.6917293233082706}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexr\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# –ó–∞–¥–∞—ë–º –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ RuBERT-tiny\n",
    "MODEL_NAME = \"cointegrated/rubert-tiny2\"\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä –∏ –º–æ–¥–µ–ª—å RuBERT-tiny –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —Ç–µ–∫—Å—Ç–∞\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    return embeddings.cpu().numpy()[0]\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç FAQ\n",
    "dataset = load_dataset(\"json\", data_files=\"computer_faq_extended_dataset.json\")[\"train\"]\n",
    "\n",
    "# –°—Ç—Ä–æ–∏–º –±–∞–∑—É: –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∏ –æ—Ç–≤–µ—Ç\n",
    "faq_database = []\n",
    "for item in dataset:\n",
    "    question_text = item[\"–≤–æ–ø—Ä–æ—Å\"]\n",
    "    answer_text = item[\"–æ—Ç–≤–µ—Ç\"]\n",
    "    emb = get_embedding(question_text)\n",
    "    faq_database.append({\n",
    "        \"question\": question_text,\n",
    "        \"answer\": answer_text,\n",
    "        \"embedding\": emb\n",
    "    })\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞ –±–ª–∏–∂–∞–π—à–µ–≥–æ –≤–æ–ø—Ä–æ—Å–∞ –≤ –±–∞–∑–µ —Å –ø–æ–º–æ—â—å—é –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞\n",
    "def find_best_answer(user_question, top_n=3, threshold=0.5):\n",
    "    user_emb = get_embedding(user_question)\n",
    "    embeddings = np.array([entry[\"embedding\"] for entry in faq_database])\n",
    "    scores = cosine_similarity([user_emb], embeddings)[0]\n",
    "\n",
    "    # –ë–µ—Ä—ë–º top_n –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤\n",
    "    top_indices = np.argsort(scores)[-top_n:][::-1]\n",
    "    top_matches = [(faq_database[i][\"answer\"], scores[i]) for i in top_indices if scores[i] > threshold]\n",
    "\n",
    "    if not top_matches:\n",
    "        return \"–ò–∑–≤–∏–Ω–∏—Ç–µ, —è –Ω–µ —Å–º–æ–≥ –Ω–∞–π—Ç–∏ –ø–æ–¥—Ö–æ–¥—è—â–∏–π –æ—Ç–≤–µ—Ç.\", 0.0\n",
    "\n",
    "    # –í—ã–±–∏—Ä–∞–µ–º –æ—Ç–≤–µ—Ç —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º —Å—Ä–µ–¥–Ω–∏–º –±–∞–ª–ª–æ–º –∏–ª–∏ –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã–π\n",
    "    best_answer = max(top_matches, key=lambda x: x[1])[0]\n",
    "    return best_answer, max(top_matches, key=lambda x: x[1])[1]\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞\n",
    "def answer_question(user_question):\n",
    "    answer, score = find_best_answer(user_question)\n",
    "    return answer, score\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫\n",
    "def compute_metrics():\n",
    "    similarities = []\n",
    "    for item in dataset:\n",
    "        predicted_answer, _ = find_best_answer(item[\"–≤–æ–ø—Ä–æ—Å\"])\n",
    "        real_emb = get_embedding(item[\"–æ—Ç–≤–µ—Ç\"])\n",
    "        pred_emb = get_embedding(predicted_answer)\n",
    "        similarity = cosine_similarity([real_emb], [pred_emb])[0][0]\n",
    "        similarities.append(similarity)\n",
    "    return {\n",
    "        \"f1\": f1_metric.compute(predictions=similarities, references=[1.0] * len(similarities), average=\"weighted\")[\"f1\"],\n",
    "        \"precision\": precision_metric.compute(predictions=similarities, references=[1.0] * len(similarities), average=\"weighted\")[\"precision\"],\n",
    "        \"recall\": recall_metric.compute(predictions=similarities, references=[1.0] * len(similarities), average=\"weighted\")[\"recall\"],\n",
    "    }\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
    "if __name__ == \"__main__\":\n",
    "    user_question = \"–ü–æ—á–µ–º—É –∫–æ–º–ø—å—é—Ç–µ—Ä –Ω–µ –≤–∫–ª—é—á–∞–µ—Ç—Å—è?\"\n",
    "    answer, score = answer_question(user_question)\n",
    "    print(\"–í–æ–ø—Ä–æ—Å:\", user_question)\n",
    "    print(\"–ù–∞–π–¥–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç:\", answer)\n",
    "    print(\"–°—Ö–æ–∂–µ—Å—Ç—å:\", score)\n",
    "    \n",
    "    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫\n",
    "    metrics = compute_metrics()\n",
    "    print(\"–ú–µ—Ç—Ä–∏–∫–∏:\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e59f763f-79b0-41b8-8b71-664107d8337a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–í–æ–ø—Ä–æ—Å: –Ø —É–¥–∞–ª–∏–ª —Ñ–∞–π–ª—ã, –∫–∞–∫ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å?\n",
      "–ù–∞–π–¥–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã Recuva –∏–ª–∏ –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ —Å—Ä–µ–¥—Å—Ç–≤–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è.\n",
      "–°—Ö–æ–∂–µ—Å—Ç—å: 0.78735596\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    user_question = \"–Ø —É–¥–∞–ª–∏–ª —Ñ–∞–π–ª—ã, –∫–∞–∫ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å?\"\n",
    "    answer, score = answer_question(user_question)\n",
    "    print(\"–í–æ–ø—Ä–æ—Å:\", user_question)\n",
    "    print(\"–ù–∞–π–¥–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç:\", answer)\n",
    "    print(\"–°—Ö–æ–∂–µ—Å—Ç—å:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "742a052e-e66e-4176-8286-34b590a8bfdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexr\\anaconda3\\envs\\my_rag\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω–∞!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "with open('computer_faq_extended_dataset.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 2. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ChromaDB\n",
    "client = chromadb.PersistentClient(path=\"chroma_db\")\n",
    "embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    ")\n",
    "collection = client.get_or_create_collection(\n",
    "    name=\"computer_faq\",\n",
    "    embedding_function=embedding_func,\n",
    ")\n",
    "\n",
    "# 3. –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É\n",
    "collection.add(\n",
    "    documents=[item[\"–æ—Ç–≤–µ—Ç\"] for item in data],\n",
    "    ids=[str(i) for i in range(len(data))],\n",
    "    metadatas=[{\"question\": item[\"–≤–æ–ø—Ä–æ—Å\"]} for item in data]\n",
    ")\n",
    "\n",
    "# 4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ (–≤ –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ - —Ç–æ–ª—å–∫–æ –≤–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞)\n",
    "print(\"–í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω–∞!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e48348cd-76b8-4d27-9588-09a396d08f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 38 key-value pairs and 291 tensors from models/Llama3-Instruct-8B-RSPO.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Llama 3 Instruct 8B RSPO\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Li11111\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct-RSPO\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Llama-3\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 8B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   9:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv  10:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"ƒ† ƒ†\", \"ƒ† ƒ†ƒ†ƒ†\", \"ƒ†ƒ† ƒ†ƒ†\", \"...\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 128009\n",
      "llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  29:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  30:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  31:                                general.url str              = https://huggingface.co/mradermacher/L...\n",
      "llama_model_loader: - kv  32:              mradermacher.quantize_version str              = 2\n",
      "llama_model_loader: - kv  33:                  mradermacher.quantized_by str              = mradermacher\n",
      "llama_model_loader: - kv  34:                  mradermacher.quantized_at str              = 2025-02-17T21:42:55+01:00\n",
      "llama_model_loader: - kv  35:                  mradermacher.quantized_on str              = rich1\n",
      "llama_model_loader: - kv  36:                         general.source.url str              = https://huggingface.co/li11111/Llama3...\n",
      "llama_model_loader: - kv  37:                  mradermacher.convert_type str              = hf\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 256/128256 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW) \n",
      "llm_load_print_meta: general.name     = Llama 3 Instruct 8B RSPO\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: PAD token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: LF token         = 30 '?'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4685.30 MiB\n",
      "........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    13.02 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   258.50 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'mradermacher.convert_type': 'hf', 'general.name': 'Llama 3 Instruct 8B RSPO', 'general.architecture': 'llama', 'general.type': 'model', 'llama.context_length': '8192', 'general.organization': 'Li11111', 'general.basename': 'Llama-3', 'general.finetune': 'Instruct-RSPO', 'mradermacher.quantized_on': 'rich1', 'general.size_label': '8B', 'general.license': 'apache-2.0', 'llama.block_count': '32', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'general.file_type': '15', 'tokenizer.ggml.eos_token_id': '128009', 'llama.attention.head_count_kv': '8', 'llama.rope.freq_base': '500000.000000', 'mradermacher.quantize_version': '2', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.attention.key_length': '128', 'llama.attention.value_length': '128', 'llama.vocab_size': '128256', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.pre': 'llama-bpe', 'general.source.url': 'https://huggingface.co/li11111/Llama3-Instruct-8B-RSPO', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.ggml.padding_token_id': '128009', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\", 'general.url': 'https://huggingface.co/mradermacher/Llama3-Instruct-8B-RSPO-GGUF', 'mradermacher.quantized_by': 'mradermacher', 'mradermacher.quantized_at': '2025-02-17T21:42:55+01:00'}\n",
      "Using gguf chat template: {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
      "\n",
      "'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}{% endif %}\n",
      "Using chat eos_token: <|eot_id|>\n",
      "Using chat bos_token: <|begin_of_text|>\n",
      "\n",
      "llama_print_timings:        load time =    5610.05 ms\n",
      "llama_print_timings:      sample time =      38.47 ms /    99 runs   (    0.39 ms per token,  2573.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5609.90 ms /    87 tokens (   64.48 ms per token,    15.51 tokens per second)\n",
      "llama_print_timings:        eval time =   15237.70 ms /    98 runs   (  155.49 ms per token,     6.43 tokens per second)\n",
      "llama_print_timings:       total time =   21673.99 ms /   185 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ù–µ–æ–±—Ö–æ–¥–∏–º–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Ñ–∏–∑–∏—á–µ—Å–∫–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø–∏—Ç–∞–Ω–∏—è –∫–æ–º–ø—å—é—Ç–µ—Ä–∞ –∏ –µ–≥–æ –±–ª–æ–∫–∏ –ø–∏—Ç–∞–Ω–∏—è. –ï—Å–ª–∏ –ø—Ä–æ–±–ª–µ–º–∞ –Ω–µ —Å–≤—è–∑–∞–Ω–∞ —Å –Ω–∏–º–∏, –º–æ–∂–Ω–æ –ø—Ä–µ–¥–ø–æ–ª–æ–∂–∏—Ç—å –ø—Ä–æ–±–ª–µ–º—É —Å —Ñ–∞–π–ª–æ–≤–æ–π —Å–∏—Å—Ç–µ–º–æ–π –∏–ª–∏ –∑–∞—Ä–∞–∂–µ–Ω–∏–µ–º –≤–∏—Ä—É—Å–æ–º. –í —ç—Ç–æ–º —Å–ª—É—á–∞–µ –ø–æ–ø—ã—Ç–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–∞–º–º—ã Recuva –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ –∫–æ–º–∞–Ω–¥–∞ `attrib` –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏ –∏–∑–º–µ–Ω–µ–Ω–∏–π –∞—Ç—Ä–∏–±—É—Ç–æ–≤ —Ñ–∞–π–ª–æ–≤ –º–æ–∂–µ—Ç –ø–æ–º–æ—á—å –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –∏ —Ä–µ—à–∏—Ç—å –ø—Ä–æ–±–ª–µ–º—É —Å –≤–∫–ª—é—á–µ–Ω–∏–µ–º –∫–æ–º–ø—å—é—Ç–µ—Ä–∞.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    5610.05 ms\n",
      "llama_print_timings:      sample time =      38.34 ms /    99 runs   (    0.39 ms per token,  2582.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3679.21 ms /    59 tokens (   62.36 ms per token,    16.04 tokens per second)\n",
      "llama_print_timings:        eval time =   15254.05 ms /    98 runs   (  155.65 ms per token,     6.42 tokens per second)\n",
      "llama_print_timings:       total time =   19743.96 ms /   157 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ù–µ–æ–±—Ö–æ–¥–∏–º–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Ñ–∞–π–ª–æ–≤—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã –Ω–∞ –¥–∏—Å–∫–µ X:, –≤–æ–∑–º–æ–∂–Ω–æ —Ñ–∞–π–ª—ã –∏–ª–∏ –ø–∞–ø–∫–∏ –∏–º–µ—é—Ç —Å–∫—Ä—ã—Ç—ã–π ('h'), —Å–∏—Å—Ç–µ–º–Ω—ã–π ('s') –∏–ª–∏ –∑–∞—â–∏—â–µ–Ω–Ω—ã–π ('r') —Å—Ç–∞—Ç—É—Å, —á—Ç–æ –º–µ—à–∞–µ—Ç –∏—Ö –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—é –Ω–∞ –º–æ–Ω–∏—Ç–æ—Ä–µ. –ò—Å–ø–æ–ª—å–∑—É–µ–º–∞—è –∫–æ–º–∞–Ω–¥–∞ 'attrib' –∏–ª–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∞ Recuva –ø–æ–∑–≤–æ–ª—è—é—Ç –∏–∑–º–µ–Ω–∏—Ç—å —ç—Ç–∏ –∞—Ç—Ä–∏–±—É—Ç—ã –∏ —Å–¥–µ–ª–∞—Ç—å —Ñ–∞–π–ª—ã –¥–æ—Å—Ç—É–ø–Ω—ã–º–∏ –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –Ω–∞ —ç–∫—Ä–∞–Ω–µ.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "client = chromadb.PersistentClient(path=\"chroma_db\")\n",
    "embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "collection = client.get_or_create_collection(\"computer_faq\")\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ Llama3\n",
    "llm = Llama(\n",
    "    model_path=\"models/Llama3-Instruct-8B-RSPO.Q4_K_M.gguf\",\n",
    "    n_ctx=2048,\n",
    "    n_threads=4\n",
    ")\n",
    "\n",
    "def get_answer(question):\n",
    "    # –®–∞–≥ 1: –ü–æ–∏—Å–∫ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ\n",
    "    results = collection.query(\n",
    "        query_texts=[question],\n",
    "        n_results=1\n",
    "    )\n",
    "    \n",
    "    exact_answer = results[\"documents\"][0][0] if results[\"documents\"] else None\n",
    "    \n",
    "    # –®–∞–≥ 2: –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏ –≤–æ–ø—Ä–æ—Å–∞\n",
    "    if results[\"distances\"][0][0] < 0.3:  # –ü–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏\n",
    "        return exact_answer\n",
    "    \n",
    "    # –®–∞–≥ 3: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –¥–ª—è –Ω–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤\n",
    "    context = \"\\n\".join(results[\"documents\"][0])\n",
    "    prompt = f\"\"\"–¢—ã IT-—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç. –û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å, –∏—Å–ø–æ–ª—å–∑—É—è –∫–æ–Ω—Ç–µ–∫—Å—Ç:\n",
    "    \n",
    "    –í–æ–ø—Ä–æ—Å: {question}\n",
    "    –ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}\n",
    "    \n",
    "    –û—Ç–≤–µ—Ç (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è):\"\"\"\n",
    "    \n",
    "    output = llm(prompt, max_tokens=150, temperature=0.2)\n",
    "    return output[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä—ã\n",
    "print(get_answer(\"–ü–æ—á–µ–º—É –∫–æ–º–ø—å—é—Ç–µ—Ä –Ω–µ –≤–∫–ª—é—á–∞–µ—Ç—Å—è?\"))  # –û—Ç–≤–µ—Ç –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c54429a8-3c07-45ef-a928-cf2355930fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5610.05 ms\n",
      "llama_print_timings:      sample time =      45.02 ms /   116 runs   (    0.39 ms per token,  2576.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   18229.32 ms /   116 runs   (  157.15 ms per token,     6.36 tokens per second)\n",
      "llama_print_timings:       total time =   19136.80 ms /   117 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ù–µ–æ–±—Ö–æ–¥–∏–º–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Ñ–∞–π–ª–æ–≤—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã –Ω–∞ –¥–∏—Å–∫–µ X:, –≤–æ–∑–º–æ–∂–Ω–æ —Ñ–∞–π–ª—ã –∏–ª–∏ –ø–∞–ø–∫–∏ –∏–º–µ—é—Ç —Å–∫—Ä—ã—Ç—ã–π ('h'), —Å–∏—Å—Ç–µ–º–Ω—ã–π ('s') –∏–ª–∏ –∑–∞—â–∏—â–µ–Ω–Ω—ã–π ('r') —Å—Ç–∞—Ç—É—Å, —á—Ç–æ –º–µ—à–∞–µ—Ç –∏—Ö –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—é –Ω–∞ –º–æ–Ω–∏—Ç–æ—Ä–µ. –ò—Å–ø–æ–ª—å–∑—É–µ–º–∞—è –∫–æ–º–∞–Ω–¥–∞ 'attrib' –∏–ª–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∞ Recuva –ø–æ–º–æ–≥—É—Ç –æ–±–Ω–∞—Ä—É–∂–∏—Ç—å –∏ –∏–∑–º–µ–Ω–∏—Ç—å —ç—Ç–∏ –∞—Ç—Ä–∏–±—É—Ç—ã, –µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ. –ó–∞—Ç–µ–º –Ω—É–∂–Ω–æ –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –ø–æ–ø—ã—Ç–∫—É –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –¥–∏—Å–∫–∞ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –º–æ–Ω–∏—Ç–æ—Ä–µ.\n"
     ]
    }
   ],
   "source": [
    "print(get_answer(\"–ü–æ—á–µ–º—É –Ω–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–∞ –º–æ–Ω–∏—Ç–æ—Ä–µ?\"))  # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–æ–≤–æ–≥–æ –æ—Ç–≤–µ—Ç–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1105aab5-0ec6-426f-b686-c4398ccb17f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexr\\anaconda3\\envs\\my_rag\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "llama_model_loader: loaded meta data with 38 key-value pairs and 291 tensors from models/Llama3-Instruct-8B-RSPO.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Llama 3 Instruct 8B RSPO\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Li11111\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct-RSPO\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Llama-3\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 8B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   9:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv  10:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"ƒ† ƒ†\", \"ƒ† ƒ†ƒ†ƒ†\", \"ƒ†ƒ† ƒ†ƒ†\", \"...\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 128009\n",
      "llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  29:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  30:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  31:                                general.url str              = https://huggingface.co/mradermacher/L...\n",
      "llama_model_loader: - kv  32:              mradermacher.quantize_version str              = 2\n",
      "llama_model_loader: - kv  33:                  mradermacher.quantized_by str              = mradermacher\n",
      "llama_model_loader: - kv  34:                  mradermacher.quantized_at str              = 2025-02-17T21:42:55+01:00\n",
      "llama_model_loader: - kv  35:                  mradermacher.quantized_on str              = rich1\n",
      "llama_model_loader: - kv  36:                         general.source.url str              = https://huggingface.co/li11111/Llama3...\n",
      "llama_model_loader: - kv  37:                  mradermacher.convert_type str              = hf\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 256/128256 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW) \n",
      "llm_load_print_meta: general.name     = Llama 3 Instruct 8B RSPO\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: PAD token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: LF token         = 30 '?'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4685.30 MiB\n",
      "........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    13.02 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   258.50 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'mradermacher.convert_type': 'hf', 'general.name': 'Llama 3 Instruct 8B RSPO', 'general.architecture': 'llama', 'general.type': 'model', 'llama.context_length': '8192', 'general.organization': 'Li11111', 'general.basename': 'Llama-3', 'general.finetune': 'Instruct-RSPO', 'mradermacher.quantized_on': 'rich1', 'general.size_label': '8B', 'general.license': 'apache-2.0', 'llama.block_count': '32', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'general.file_type': '15', 'tokenizer.ggml.eos_token_id': '128009', 'llama.attention.head_count_kv': '8', 'llama.rope.freq_base': '500000.000000', 'mradermacher.quantize_version': '2', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.attention.key_length': '128', 'llama.attention.value_length': '128', 'llama.vocab_size': '128256', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.pre': 'llama-bpe', 'general.source.url': 'https://huggingface.co/li11111/Llama3-Instruct-8B-RSPO', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.ggml.padding_token_id': '128009', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\", 'general.url': 'https://huggingface.co/mradermacher/Llama3-Instruct-8B-RSPO-GGUF', 'mradermacher.quantized_by': 'mradermacher', 'mradermacher.quantized_at': '2025-02-17T21:42:55+01:00'}\n",
      "Using gguf chat template: {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
      "\n",
      "'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}{% endif %}\n",
      "Using chat eos_token: <|eot_id|>\n",
      "Using chat bos_token: <|begin_of_text|>\n",
      "\n",
      "llama_print_timings:        load time =   12542.39 ms\n",
      "llama_print_timings:      sample time =      35.99 ms /    91 runs   (    0.40 ms per token,  2528.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12542.06 ms /   217 tokens (   57.80 ms per token,    17.30 tokens per second)\n",
      "llama_print_timings:        eval time =   14609.96 ms /    90 runs   (  162.33 ms per token,     6.16 tokens per second)\n",
      "llama_print_timings:       total time =   27928.54 ms /   307 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ú–æ–Ω–∏—Ç–æ—Ä –º–æ–∂–µ—Ç –Ω–µ –æ—Ç–æ–±—Ä–∞–∂–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑-–∑–∞ –æ—à–∏–±–∫–∏ –¥—Ä–∞–π–≤–µ—Ä–∞ –≤–∏–¥–µ–æ–∫–∞—Ä—Ç—ã, –ø—Ä–æ–±–ª–µ–º—ã —Å–æ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ–º –≤–∏–¥–µ–æ–∞–¥–∞–ø—Ç–µ—Ä–∞ –∏–ª–∏ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏—è –º–æ–Ω–∏—Ç–æ—Ä–∞ –∏–ª–∏ –∫–∞–±–µ–ª—è VGA/HDMI. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∏ –æ–±–Ω–æ–≤–∏—Ç–µ –¥—Ä–∞–π–≤–µ—Ä –≤–∏–¥–µ–æ–∫–∞—Ä—Ç—ã, –µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ. –ï—Å–ª–∏ –ø—Ä–æ–±–ª–µ–º–∞ persists, –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –ø–æ–¥–¥–µ—Ä–∂–∫–µ –∏–ª–∏ –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –º–æ–Ω–∏—Ç–æ—Ä –Ω–∞ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏—è.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   12542.39 ms\n",
      "llama_print_timings:      sample time =      30.04 ms /    75 runs   (    0.40 ms per token,  2497.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9226.39 ms /   167 tokens (   55.25 ms per token,    18.10 tokens per second)\n",
      "llama_print_timings:        eval time =   11909.29 ms /    74 runs   (  160.94 ms per token,     6.21 tokens per second)\n",
      "llama_print_timings:       total time =   21742.13 ms /   241 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–î–ª—è –ø–æ—á–∏–Ω–∫–∏ –ø—Ä–∏–Ω—Ç–µ—Ä–∞ –ø—Ä–æ–≤–µ—Ä—å—Ç–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –∫–∞–±–µ–ª–µ–π, —É–±–µ–¥–∏—Ç–µ—Å—å –≤ –Ω–∞–ª–∏—á–∏–∏ –ø–æ–¥—Ö–æ–¥—è—â–µ–≥–æ –¥—Ä–∞–π–≤–µ—Ä–∞ –∏ –æ–±–Ω–æ–≤–∏—Ç–µ –µ–≥–æ –µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —Ç—Ä–µ–±—É–µ—Ç. –ï—Å–ª–∏ –ø—Ä–æ–±–ª–µ–º–∞ persists, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –æ—á–∏—Å—Ç–∫—É –ø—Ä–∏–Ω—Ç–µ—Ä–Ω–æ–π –≥–æ–ª–æ–≤–∫–∏ –∏–ª–∏ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–º—É –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏—é –∏–ª–∏ –∫–æ–Ω—Ç–∞–∫—Ç—É—Ä—É—Ç–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   12542.39 ms\n",
      "llama_print_timings:      sample time =      40.04 ms /   101 runs   (    0.40 ms per token,  2522.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9952.34 ms /   181 tokens (   54.99 ms per token,    18.19 tokens per second)\n",
      "llama_print_timings:        eval time =   16198.01 ms /   100 runs   (  161.98 ms per token,     6.17 tokens per second)\n",
      "llama_print_timings:       total time =   27009.71 ms /   281 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ï—Å–ª–∏ –∫–æ–º–ø—å—é—Ç–µ—Ä –Ω–µ –≤–∫–ª—é—á–∞–µ—Ç—Å—è, –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –∏—Å—Ç–æ—á–Ω–∏–∫ –ø–∏—Ç–∞–Ω–∏—è (–∑–∞—Ä—è–¥ –±–∞—Ç–∞—Ä–µ–∏ –∏–ª–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ —Å–µ—Ç–∏), –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –±–ª–æ–∫ –ø–∏—Ç–∞–Ω–∏—è –∏ –∫–∞–±–µ–ª–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –º–æ–Ω–∏—Ç–æ—Ä—É –∏ –ø–µ—Ä–∏—Ñ–µ—Ä–∏–π–Ω—ã–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞–º. –í —Å–ª—É—á–∞–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ —Å–±—Ä–æ—Å —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –∏–ª–∏ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—é –∏–∑ –∑–∞–≤–æ–¥—Å–∫–∏—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—è. –ï—Å–ª–∏ –ø—Ä–æ–±–ª–µ–º–∞ persists, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –∫ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—É –∏–ª–∏ –∫–æ–Ω—Ç–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å —Å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—è.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "client = chromadb.PersistentClient(path=\"chroma_db\")\n",
    "embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "collection = client.get_or_create_collection(\"computer_faq\")\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ Llama3\n",
    "llm = Llama(\n",
    "    model_path=\"models/Llama3-Instruct-8B-RSPO.Q4_K_M.gguf\",\n",
    "    n_ctx=2048,\n",
    "    n_threads=4\n",
    ")\n",
    "\n",
    "def get_answer(question):\n",
    "    # –®–∞–≥ 1: –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤\n",
    "    results = collection.query(\n",
    "        query_texts=[question],\n",
    "        n_results=3  # –ë–µ—Ä–µ–º –±–æ–ª—å—à–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞\n",
    "    )\n",
    "    \n",
    "    # –®–∞–≥ 2: –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (–±–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∏–π –ø–æ—Ä–æ–≥)\n",
    "    if results[\"distances\"][0][0] < 0.5:  # –ë–æ–ª–µ–µ –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥\n",
    "        best_answer = results[\"documents\"][0][0]\n",
    "        best_question = results[\"metadatas\"][0][0][\"question\"]\n",
    "        \n",
    "        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏ –≤–æ–ø—Ä–æ—Å–∞\n",
    "        if question.lower() in best_question.lower() or best_question.lower() in question.lower():\n",
    "            return best_answer\n",
    "    \n",
    "    # –®–∞–≥ 3: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏\n",
    "    context = \"\\n\".join([\n",
    "        f\"–í–æ–ø—Ä–æ—Å: {q['question']}\\n–û—Ç–≤–µ—Ç: {a}\" \n",
    "        for q, a in zip(results[\"metadatas\"][0], results[\"documents\"][0])\n",
    "    ])\n",
    "    \n",
    "    # –®–∞–≥ 4: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å —á–µ—Ç–∫–∏–º–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏\n",
    "    prompt = f\"\"\"–¢—ã –æ–ø—ã—Ç–Ω—ã–π IT-—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç. –û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É.\n",
    "–ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –Ω–µ —Å–≤—è–∑–∞–Ω —Å IT, –≤–µ–∂–ª–∏–≤–æ —Å–æ–æ–±—â–∏ –æ–± —ç—Ç–æ–º.\n",
    "\n",
    "–í–æ–ø—Ä–æ—Å: {question}\n",
    "\n",
    "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∑–Ω–∞–Ω–∏—è –∏–∑ –±–∞–∑—ã:\n",
    "{context}\n",
    "\n",
    "–û—Ç–≤–µ—Ç (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, —Ç–æ–ª—å–∫–æ –ø–æ —Å—É—â–µ—Å—Ç–≤—É):\"\"\"\n",
    "    \n",
    "    output = llm(\n",
    "        prompt,\n",
    "        max_tokens=150,\n",
    "        temperature=0.1,  # –ú–µ–Ω—å—à–µ \"–∫—Ä–µ–∞—Ç–∏–≤–∞\"\n",
    "        stop=[\"\\n\", \"–í–æ–ø—Ä–æ—Å:\", \"–ö–æ–Ω—Ç–µ–∫—Å—Ç:\"]\n",
    "    )\n",
    "    \n",
    "    return output[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä—ã\n",
    "print(get_answer(\"–ü–æ—á–µ–º—É –Ω–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–∞ –º–æ–Ω–∏—Ç–æ—Ä–µ?\"))\n",
    "print(get_answer(\"–ö–∞–∫ –ø–æ—á–∏–Ω–∏—Ç—å –ø—Ä–∏–Ω—Ç–µ—Ä?\"))\n",
    "print(get_answer(\"–ß—Ç–æ –¥–µ–ª–∞—Ç—å, –µ—Å–ª–∏ –∫–æ–º–ø—å—é—Ç–µ—Ä –Ω–µ –≤–∫–ª—é—á–∞–µ—Ç—Å—è?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2dfd70e-e8a8-4f3b-a341-8babd1b72606",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12542.39 ms\n",
      "llama_print_timings:      sample time =      25.64 ms /    65 runs   (    0.39 ms per token,  2535.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10022.85 ms /   184 tokens (   54.47 ms per token,    18.36 tokens per second)\n",
      "llama_print_timings:        eval time =   10070.51 ms /    64 runs   (  157.35 ms per token,     6.36 tokens per second)\n",
      "llama_print_timings:       total time =   20617.44 ms /   248 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–§–∞–π–ª–æ–≤—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –∏–ª–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∫–∞ –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, Recuva) –ø–æ–º–æ–≥—É—Ç –Ω–∞–π—Ç–∏ –∏ –≤–µ—Ä–Ω—É—Ç—å –ø—Ä–æ–ø–∞–≤—à–∏–µ —Ñ–∞–π–ª—ã —Å —Ñ–ª–µ—à–∫–∏. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —Ñ–ª–µ—à–∫–∏ –∏ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –µ–µ —Ñ–∞–π–ª–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã –ø–µ—Ä–µ–¥ –ø–æ–ø—ã—Ç–∫–æ–π –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö.\n"
     ]
    }
   ],
   "source": [
    "print(get_answer(\"–ú–∏–∫—Ä–æ—Ñ–æ–Ω –≤—ã–∫–ª—é—á–∏–ª—Å—è –≤ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π –º–æ–º–µ–Ω—Ç\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0d3e28ae-cc32-4a6d-9883-101bd2c533e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 38 key-value pairs and 291 tensors from models/Llama3-Instruct-8B-RSPO.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Llama 3 Instruct 8B RSPO\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Li11111\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct-RSPO\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Llama-3\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 8B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   9:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv  10:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"ƒ† ƒ†\", \"ƒ† ƒ†ƒ†ƒ†\", \"ƒ†ƒ† ƒ†ƒ†\", \"...\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 128009\n",
      "llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  29:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  30:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  31:                                general.url str              = https://huggingface.co/mradermacher/L...\n",
      "llama_model_loader: - kv  32:              mradermacher.quantize_version str              = 2\n",
      "llama_model_loader: - kv  33:                  mradermacher.quantized_by str              = mradermacher\n",
      "llama_model_loader: - kv  34:                  mradermacher.quantized_at str              = 2025-02-17T21:42:55+01:00\n",
      "llama_model_loader: - kv  35:                  mradermacher.quantized_on str              = rich1\n",
      "llama_model_loader: - kv  36:                         general.source.url str              = https://huggingface.co/li11111/Llama3...\n",
      "llama_model_loader: - kv  37:                  mradermacher.convert_type str              = hf\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 256/128256 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW) \n",
      "llm_load_print_meta: general.name     = Llama 3 Instruct 8B RSPO\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: PAD token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: LF token         = 30 '?'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4685.30 MiB\n",
      "........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    17.04 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   288.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'mradermacher.convert_type': 'hf', 'general.name': 'Llama 3 Instruct 8B RSPO', 'general.architecture': 'llama', 'general.type': 'model', 'llama.context_length': '8192', 'general.organization': 'Li11111', 'general.basename': 'Llama-3', 'general.finetune': 'Instruct-RSPO', 'mradermacher.quantized_on': 'rich1', 'general.size_label': '8B', 'general.license': 'apache-2.0', 'llama.block_count': '32', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'general.file_type': '15', 'tokenizer.ggml.eos_token_id': '128009', 'llama.attention.head_count_kv': '8', 'llama.rope.freq_base': '500000.000000', 'mradermacher.quantize_version': '2', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.attention.key_length': '128', 'llama.attention.value_length': '128', 'llama.vocab_size': '128256', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.pre': 'llama-bpe', 'general.source.url': 'https://huggingface.co/li11111/Llama3-Instruct-8B-RSPO', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.ggml.padding_token_id': '128009', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\", 'general.url': 'https://huggingface.co/mradermacher/Llama3-Instruct-8B-RSPO-GGUF', 'mradermacher.quantized_by': 'mradermacher', 'mradermacher.quantized_at': '2025-02-17T21:42:55+01:00'}\n",
      "Using gguf chat template: {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
      "\n",
      "'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}{% endif %}\n",
      "Using chat eos_token: <|eot_id|>\n",
      "Using chat bos_token: <|begin_of_text|>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "client = chromadb.PersistentClient(path=\"chroma_db\")\n",
    "embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "collection = client.get_or_create_collection(\"computer_faq\")\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ Llama3 —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏\n",
    "llm = Llama(\n",
    "    model_path=\"models/Llama3-Instruct-8B-RSPO.Q4_K_M.gguf\",\n",
    "    n_ctx=4096,  # –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç\n",
    "    n_threads=6,\n",
    "    n_gpu_layers=50  # –ï—Å–ª–∏ –µ—Å—Ç—å GPU\n",
    ")\n",
    "\n",
    "def get_answer(question):\n",
    "    # 1. –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (–¥–∞–∂–µ —Å–ª–∞–±—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è)\n",
    "    results = collection.query(\n",
    "        query_texts=[question],\n",
    "        n_results=5,\n",
    "        include=[\"documents\", \"metadatas\"]\n",
    "    )\n",
    "    \n",
    "    # 2. –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞\n",
    "    context = \"–í–æ–∑–º–æ–∂–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π:\\n\"\n",
    "    for meta, doc in zip(results[\"metadatas\"][0], results[\"documents\"][0]):\n",
    "        context += f\"- {meta['question']}: {doc}\\n\"\n",
    "    \n",
    "    # 3. –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Å \"—Å—Ç—Ä–∞—Ö–æ–≤–∫–æ–π\" –¥–ª—è –ª—é–±—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤\n",
    "    prompt = f\"\"\"–¢—ã –æ–ø—ã—Ç–Ω—ã–π IT-—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç. –û—Ç–≤–µ—á–∞–π –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã, –∏—Å–ø–æ–ª—å–∑—É—è:\n",
    "1. –ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π (–µ—Å–ª–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω)\n",
    "2. –°–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–µ –∑–Ω–∞–Ω–∏—è\n",
    "3. –õ–æ–≥–∏—á–µ—Å–∫–∏–µ –≤—ã–≤–æ–¥—ã\n",
    "\n",
    "–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n",
    "{context}\n",
    "\n",
    "–í–æ–ø—Ä–æ—Å: {question}\n",
    "\n",
    "–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ—Ç–≤–µ—Ç–∞:\n",
    "1. –û—Å–Ω–æ–≤–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)\n",
    "2. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)\n",
    "3. –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã (–µ—Å–ª–∏ –ø—Ä–∏–º–µ–Ω–∏–º–æ)\n",
    "\n",
    "–û—Ç–≤–µ—Ç:\"\"\"\n",
    "    \n",
    "    # –ü–µ—Ä–≤–∏—á–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è\n",
    "    output = llm(prompt, max_tokens=500, temperature=0.3)\n",
    "    answer = output[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "    # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ –µ—Å–ª–∏ –æ—Ç–≤–µ—Ç –Ω–µ–ø–æ–ª–Ω—ã–π\n",
    "    if not answer.count('\\n') >= 3:  # –ú–µ–Ω–µ–µ 3 –ø—É–Ω–∫—Ç–æ–≤\n",
    "        continuation = llm(\n",
    "            f\"–ó–∞–≤–µ—Ä—à–∏ –æ—Ç–≤–µ—Ç:\\n{answer}\",\n",
    "            max_tokens=200,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        answer += \"\\n\" + continuation[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b2ebac7-552a-4930-a704-d97aa0ea10f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   18819.44 ms\n",
      "llama_print_timings:      sample time =      46.91 ms /   121 runs   (    0.39 ms per token,  2579.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18819.18 ms /   345 tokens (   54.55 ms per token,    18.33 tokens per second)\n",
      "llama_print_timings:        eval time =   19245.16 ms /   120 runs   (  160.38 ms per token,     6.24 tokens per second)\n",
      "llama_print_timings:       total time =   39050.44 ms /   465 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ü—Ä–∏ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–æ–º –≤—ã–∫–ª—é—á–µ–Ω–∏–∏ –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞ –≤ Windows —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –µ–≥–æ —Å–≤–æ–π—Å—Ç–≤–∞ –≤ –ü–∞–Ω–µ–ª–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∑–≤—É–∫–æ–º (–Ω–∞–ø—Ä–∏–º–µ—Ä, —á–µ—Ä–µ–∑ ¬´–°onus¬ª –∏–ª–∏ ¬´–ó–≤—É–∫¬ª –≤ –ü–∞–Ω–µ–ª–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è). –ü–µ—Ä–µ–π–¥–∏—Ç–µ –≤ —Ä–∞–∑–¥–µ–ª ¬´Recording devices¬ª –∏–ª–∏ ¬´–ú–∏–∫—Ä–æ—Ñ–æ–Ω—ã¬ª, –Ω–∞–π–¥–∏—Ç–µ –≤—ã–∫–ª—é—á–µ–Ω–Ω—ã–π –º–∏–∫—Ä–æ—Ñ–æ–Ω –∏ —É–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –æ–Ω –Ω–µ –æ—Ç–∫–ª—é—á–µ–Ω –∏–ª–∏ –Ω–µ –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω. –ï—Å–ª–∏ –ø—Ä–æ–±–ª–µ–º–∞ persists, –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –¥—Ä–∞–π–≤–µ—Ä–∞ –∑–≤—É–∫–æ–≤–æ–≥–æ –∞–¥–∞–ø—Ç–µ—Ä–∞ –∏ –æ–±–Ω–æ–≤–∏—Ç–µ –∏—Ö, –µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —Ç—Ä–µ–±—É–µ—Ç.\n"
     ]
    }
   ],
   "source": [
    "print(get_answer(\"–ú–∏–∫—Ä–æ—Ñ–æ–Ω –≤—ã–∫–ª—é—á–∏–ª—Å—è –≤ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π –º–æ–º–µ–Ω—Ç\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "279897c9-b5c9-468b-9647-3eeca2272081",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   18428.48 ms\n",
      "llama_print_timings:      sample time =      82.70 ms /   212 runs   (    0.39 ms per token,  2563.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14720.60 ms /   269 tokens (   54.72 ms per token,    18.27 tokens per second)\n",
      "llama_print_timings:        eval time =   33750.27 ms /   211 runs   (  159.95 ms per token,     6.25 tokens per second)\n",
      "llama_print_timings:       total time =   50250.83 ms /   480 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–î–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ RGB-–ø–æ–¥—Å–≤–µ—Ç–∫–∏ –Ω–∞ –Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π –∫–ª–∞–≤–∏–∞—Ç—É—Ä–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–µ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏–µ –æ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—è –∫–ª–∞–≤–∏–∞—Ç—É—Ä—ã –∏–ª–∏ —Ç—Ä–µ—Ç—å–∏—Ö —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤,such as Keyboard Lighting Software –∏–ª–∏ similar tools.\n",
      "\n",
      "–ü–µ—Ä–µ–¥ –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π –ø—Ä–æ–≤–µ—Ä—å—Ç–µ,ÊòØÂê¶ –∫–ª–∞–≤–∏–∞—Ç—É—Ä–∞ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç RGB-–ø–æ–¥—Å–≤–µ—Ç–∫—É –∏ –∏–º–µ–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è –µ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è. –í –ø—Ä–æ—Ç–∏–≤–Ω–æ–º —Å–ª—É—á–∞–µ –º–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å—Å—è –ø–æ–∏—Å–∫ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è –∏–ª–∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—è –∫–ª–∞–≤–∏–∞—Ç—É—Ä—ã –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–¥—Å–≤–µ—Ç–∫–∏.\n",
      "\n",
      "–í –∫–∞—á–µ—Å—Ç–≤–µ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã –º–æ–∂–Ω–æ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤—Å–µ–æ–±—â–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ—Å–≤–µ—â–µ–Ω–∏–µ–º, —Ç–∞–∫–∏–µ –∫–∞–∫ RGB-control software –∏–ª–∏ –¥–∞–∂–µ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –º–µ–¥–∏–∞–ø–ª—ç–µ—Ä—ã —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –æ—Å–≤–µ—â–µ–Ω–∏—è –¥–ª—è –∫–ª–∞–≤–∏–∞—Ç—É—Ä (–µ—Å–ª–∏ –æ–Ω–∏ —Å–æ–≤–º–µ—Å—Ç–∏–º—ã —Å –≤–∞—à–µ–π –∫–ª–∞–≤–∏–∞—Ç—É—Ä–æ–π). –û–¥–Ω–∞–∫–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Ç–∞–∫–∏—Ö —Ä–µ—à–µ–Ω–∏–π –º–æ–∂–µ—Ç –∑–∞–≤–∏—Å–µ—Ç—å –æ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏ –∫–ª–∞–≤–∏–∞—Ç—É—Ä—ã –∏ –µ–µ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è.\n"
     ]
    }
   ],
   "source": [
    "print(get_answer(\"–ö–∞–∫ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å RGB-–ø–æ–¥—Å–≤–µ—Ç–∫—É –Ω–∞ –Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π –∫–ª–∞–≤–∏–∞—Ç—É—Ä–µ?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ac9e1c2-b675-4ed8-889c-59ac24ee5f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   18819.44 ms\n",
      "llama_print_timings:      sample time =      20.18 ms /    52 runs   (    0.39 ms per token,  2576.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9573.38 ms /   172 tokens (   55.66 ms per token,    17.97 tokens per second)\n",
      "llama_print_timings:        eval time =    8042.15 ms /    51 runs   (  157.69 ms per token,     6.34 tokens per second)\n",
      "llama_print_timings:       total time =   18028.05 ms /   223 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ü—Ä–∏ —Ä–∞–±–æ—Ç–µ –±–µ—Å–ø—Ä–æ–≤–æ–¥–Ω–æ–π –º—ã—à–∏ –Ω–∞ —Å—Ç–µ–∫–ª—è–Ω–Ω–æ–º —Å—Ç–æ–ª–µ –º–æ–∂–µ—Ç –≤–æ–∑–Ω–∏–∫–∞—Ç—å –¥–µ—Ä–≥–∞–Ω–∏–µ –∏–∑-–∑–∞ —Å–ª–∞–±–æ–≥–æ —Å–∏–≥–Ω–∞–ª–∞ —Ä–∞–¥–∏–æ—Å–∏–≥–Ω–∞–ª–∞ –∫ –ø—Ä–∏–µ–º–Ω–∏–∫—É –º—ã—à–∏ –∏–ª–∏ –ø–æ–º–µ—Ö–∏ –æ—Ç –æ—Ç—Ä–∞–∂–µ–Ω–∏—è —Ä–∞–¥–∏–æ–≤–æ–ª–Ω –æ—Ç –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–∏ —Å—Ç–æ–ª–∞.\n"
     ]
    }
   ],
   "source": [
    "print(get_answer(\"–ü–æ—á–µ–º—É –º–æ—è –±–µ—Å–ø—Ä–æ–≤–æ–¥–Ω–∞—è –º—ã—à—å –¥–µ—Ä–≥–∞–µ—Ç—Å—è –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ –Ω–∞ —Å—Ç–µ–∫–ª—è–Ω–Ω–æ–º —Å—Ç–æ–ª–µ?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "acdfbef8-20a1-4491-9bc2-bd0e8c604bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   18819.44 ms\n",
      "llama_print_timings:      sample time =      26.26 ms /    69 runs   (    0.38 ms per token,  2627.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14104.98 ms /   256 tokens (   55.10 ms per token,    18.15 tokens per second)\n",
      "llama_print_timings:        eval time =   10734.17 ms /    68 runs   (  157.86 ms per token,     6.33 tokens per second)\n",
      "llama_print_timings:       total time =   25399.79 ms /   324 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–î–ª—è –ø–µ—Ä–µ–Ω–æ—Å–∞ Windows —Å HDD –Ω–∞ NVMe SSD –±–µ–∑ –ø–µ—Ä–µ—É—Å—Ç–∞–Ω–æ–≤–∫–∏ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç ¬´Disk Clone¬ª –∏–ª–∏ ¬´Disk Imaging¬ª –ø—Ä–æ–≥—Ä–∞–º–º—ã Acronis True Image –∏–ª–∏ EaseUS Todo Backup. –≠—Ç–∏ –ø—Ä–æ–≥—Ä–∞–º–º—ã –ø–æ–∑–≤–æ–ª—è—é—Ç —Å–æ–∑–¥–∞—Ç—å —Ç–æ—á–Ω—É—é –∫–æ–ø–∏—é –∂–µ—Å—Ç–∫–æ–≥–æ –¥–∏—Å–∫–∞ HDD –∏ –∑–∞–ø–∏—Å–∞—Ç—å –µ–µ –Ω–∞ –Ω–æ–≤—ã–π SSD NVMe.\n"
     ]
    }
   ],
   "source": [
    "print(get_answer(\"–ö–∞–∫ –ø–µ—Ä–µ–Ω–µ—Å—Ç–∏ Windows —Å HDD –Ω–∞ NVMe SSD –±–µ–∑ –ø–µ—Ä–µ—É—Å—Ç–∞–Ω–æ–≤–∫–∏?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6511da76-fb7f-4ade-a9f7-ab2b2298afdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   18819.44 ms\n",
      "llama_print_timings:      sample time =      19.50 ms /    51 runs   (    0.38 ms per token,  2615.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12404.23 ms /   225 tokens (   55.13 ms per token,    18.14 tokens per second)\n",
      "llama_print_timings:        eval time =    7906.86 ms /    50 runs   (  158.14 ms per token,     6.32 tokens per second)\n",
      "llama_print_timings:       total time =   20717.98 ms /   275 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ü—Ä–∏ –ø–æ—è–≤–ª–µ–Ω–∏–∏ —Ä–∞–∑–º—ã—Ç–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ª–∏—Ü–∞ –≤ Zoom –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–≥–æ —Ñ–æ–Ω–∞ –≤–æ–∑–º–æ–∂–Ω–æ–π –ø—Ä–∏—á–∏–Ω–æ–π —è–≤–ª—è–µ—Ç—Å—è –Ω–∏–∑–∫–∞—è —Ä–∞–∑—Ä–µ—à–∞—é—â–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –≤–∏–¥–µ–æ–∫–∞–º–µ—Ä—ã –∏–ª–∏ –Ω–µ–æ–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–∞–º–µ—Ä—ã –≤ —Å–∏—Å—Ç–µ–º–µ.\n"
     ]
    }
   ],
   "source": [
    "print(get_answer(\"–ü–æ—á–µ–º—É –≤ Zoom –º–æ–µ –ª–∏—Ü–æ –≤—ã–≥–ª—è–¥–∏—Ç —Ä–∞–∑–º—ã—Ç—ã–º –ø—Ä–∏ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–º —Ñ–æ–Ω–µ?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c0bfb7c6-9fdc-4ddd-b3f3-f69e5ced17e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   18819.44 ms\n",
      "llama_print_timings:      sample time =      27.36 ms /    70 runs   (    0.39 ms per token,  2558.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =   13191.23 ms /   238 tokens (   55.43 ms per token,    18.04 tokens per second)\n",
      "llama_print_timings:        eval time =   10915.43 ms /    69 runs   (  158.19 ms per token,     6.32 tokens per second)\n",
      "llama_print_timings:       total time =   24676.71 ms /   307 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ü—Ä–∏ –∑–∞—Ä—è–∂–µ–Ω–∏–∏ –Ω–æ—É—Ç–±—É–∫–∞ —Ç–æ–ª—å–∫–æ –≤ –æ–¥–Ω–æ–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–º –ø–æ–ª–æ–∂–µ–Ω–∏–∏ –∫–∞–±–µ–ª—è –º–æ–∂–µ—Ç –≤–æ–∑–Ω–∏–∫–Ω—É—Ç—å –ø—Ä–æ–±–ª–µ–º–∞ —Å –∫–æ–Ω—Ç–∞–∫—Ç–∞–º–∏ –∑–∞–∂–∏–º–æ–≤ –∏–ª–∏ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–µ–º –≥–∏–±–∫–æ–≥–æ –∫–∞–±–µ–ª—è. –û—Å–Ω–æ–≤–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ consists in –ø—Ä–æ–≤–µ—Ä–∫–µ –∏ –æ—á–∏—Å—Ç–∫–µ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤ –∑–∞–∂–∏–º–æ–≤ –æ—Ç –ø—ã–ª–∏ –∏ –≥—Ä—è–∑–∏ –∏–ª–∏ –∑–∞–º–µ–Ω–æ–π –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω–æ–≥–æ –∫–∞–±–µ–ª—è.\n"
     ]
    }
   ],
   "source": [
    "print(get_answer(\"–ß—Ç–æ –¥–µ–ª–∞—Ç—å, –µ—Å–ª–∏ –Ω–æ—É—Ç–±—É–∫ –∑–∞—Ä—è–∂–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –≤ –æ–¥–Ω–æ–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–º –ø–æ–ª–æ–∂–µ–Ω–∏–∏ –∫–∞–±–µ–ª—è?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4059952c-2ade-45eb-85fc-4c774f245307",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   18819.44 ms\n",
      "llama_print_timings:      sample time =      17.65 ms /    46 runs   (    0.38 ms per token,  2605.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4730.77 ms /    80 tokens (   59.13 ms per token,    16.91 tokens per second)\n",
      "llama_print_timings:        eval time =    7268.62 ms /    45 runs   (  161.52 ms per token,     6.19 tokens per second)\n",
      "llama_print_timings:       total time =   12380.63 ms /   125 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–î–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Ü–≤–µ—Ç–æ–ø–µ—Ä–µ–¥–∞—á–∏ –º–æ–Ω–∏—Ç–æ—Ä–∞ –¥–ª—è –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ —Ñ–æ—Ç–æ—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏–∑–º–µ–Ω–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Ü–≤–µ—Ç–æ–≤–æ–≥–æ –ø—Ä–æ—Ñ–∏–ª—è –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö –º–æ–Ω–∏—Ç–æ—Ä–∞ –∏–ª–∏ –≤–∏–¥–µ–æ–∫–∞—Ä—Ç—ã.\n"
     ]
    }
   ],
   "source": [
    "print(get_answer(\"–ö–∞–∫ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å —Ü–≤–µ—Ç–æ–ø–µ—Ä–µ–¥–∞—á—É –º–æ–Ω–∏—Ç–æ—Ä–∞ –¥–ª—è –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ —Ñ–æ—Ç–æ—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "24fd4135-4361-44fc-a3d5-d6192dcfdd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   18428.48 ms\n",
      "llama_print_timings:      sample time =     106.87 ms /   272 runs   (    0.39 ms per token,  2545.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15100.96 ms /   280 tokens (   53.93 ms per token,    18.54 tokens per second)\n",
      "llama_print_timings:        eval time =   43126.36 ms /   271 runs   (  159.14 ms per token,     6.28 tokens per second)\n",
      "llama_print_timings:       total time =   60500.39 ms /   551 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ü—Ä–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–∏ Bluetooth-–Ω–∞—É—à–Ω–∏–∫–æ–≤ –∫ —Ç–µ–ª–µ—Ñ–æ–Ω—É –æ–Ω–∏ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é—Ç –ø–∞—Ä–∫–æ–≤–∫—É —Å —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ–º –∏ –æ–±–º–µ–Ω–∏–≤–∞—é—Ç—Å—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–∏. –û–¥–Ω–∞–∫–æ –≤ Windows –º–æ–∂–µ—Ç –≤–æ–∑–Ω–∏–∫–Ω—É—Ç—å –ø—Ä–æ–±–ª–µ–º–∞ —Å –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤ Bluetooth –∏–∑-–∑–∞ —Å–ª–µ–¥—É—é—â–∏—Ö –ø—Ä–∏—á–∏–Ω:\n",
      "\n",
      "–û—Å–Ω–æ–≤–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ: –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–∞–Ω–µ–ª—å —É—Å—Ç—Ä–æ–π—Å—Ç–≤ Bluetooth –≤ Windows (¬´–£—Å—Ç—Ä–æ–π—Å—Ç–≤–∞¬ª ‚Üí ¬´–ëluetooth¬ª –∏–ª–∏ —á–µ—Ä–µ–∑ —Ü–µ–Ω—Ç—Ä —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è Bluetooth –≤ –ü–∞–Ω–µ–ª–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è). –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ Bluetooth-—Å–µ—Ä–≤–∏—Å –∞–∫—Ç–∏–≤–µ–Ω –∏ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –Ω–∞—É—à–Ω–∏–∫–æ–≤ –æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç—Å—è –≤ —Å–ø–∏—Å–∫–µ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤.\n",
      "\n",
      "–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏: –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ —É—Ä–æ–≤–µ–Ω—å —Å–∏–≥–Ω–∞–ª–∞ Bluetooth-—Å–∏–≥–Ω–∞–ª–∞ –≤ –Ω–∞—É—à–Ω–∏–∫–∞—Ö –∏ –Ω–∞ –∫–æ–º–ø—å—é—Ç–µ—Ä–µ. –ï—Å–ª–∏ —Å–∏–≥–Ω–∞–ª —Å–ª–∞–±—ã–π, –ø–æ–ø—ã—Ç–∞–π—Ç–µ—Å—å –ø–µ—Ä–µ–º–µ—Å—Ç–∏—Ç—å —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –±–ª–∏–∂–µ –∫ –∫–æ–º–ø—å—é—Ç–µ—Ä—É –∏–ª–∏ –∏–∑–º–µ–Ω–∏—Ç—å –ø–æ–∑–∏—Ü–∏—é –∞–Ω—Ç–µ–Ω–Ω—ã –≤ –Ω–∞—É—à–Ω–∏–∫–∞—Ö.\n",
      "\n",
      "–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã: –ï—Å–ª–∏ –ø—Ä–æ–±–ª–µ–º–∞ persists, –º–æ–∂–Ω–æ –ø–æ–ø—ã—Ç–∞—Ç—å—Å—è –æ–±–Ω–æ–≤–∏—Ç—å –¥—Ä–∞–π–≤–µ—Ä–∞ Bluetooth –≤ —Å–∏—Å—Ç–µ–º–µ –∏–ª–∏ –¥–µ–∏–Ω—Å—Ç–∞–ª–ª–∏—Ä–æ–≤–∞—Ç—å –∏ÂÜç–∏–Ω—Å—Ç–∞–ª–ª–∏—Ä–æ–≤–∞—Ç—å —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –Ω–∞—É—à–Ω–∏–∫–æ–≤ –≤ Windows Device Manager. –í –∫—Ä–∞–π–Ω–µ–º —Å–ª—É—á–∞–µ, –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—é –Ω–∞—É—à–Ω–∏–∫–æ–≤ –∑–∞ –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∏–ª–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è–º–∏ –¥—Ä–∞–π–≤–µ—Ä–æ–≤ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏—Ö –¥–ª—è –≤–∞—à–µ–≥–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞.\n"
     ]
    }
   ],
   "source": [
    "print(get_answer(\"–ü–æ—á–µ–º—É Bluetooth-–Ω–∞—É—à–Ω–∏–∫–∏ –ø–æ–¥–∫–ª—é—á–∞—é—Ç—Å—è –∫ —Ç–µ–ª–µ—Ñ–æ–Ω—É, –Ω–æ –Ω–µ –≤–∏–¥—è—Ç Windows?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a501f6ec-0210-4b78-a386-d5b349334393",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   18428.48 ms\n",
      "llama_print_timings:      sample time =     106.79 ms /   276 runs   (    0.39 ms per token,  2584.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18428.08 ms /   339 tokens (   54.36 ms per token,    18.40 tokens per second)\n",
      "llama_print_timings:        eval time =   43793.24 ms /   275 runs   (  159.25 ms per token,     6.28 tokens per second)\n",
      "llama_print_timings:       total time =   64512.53 ms /   614 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–î–ª—è —Ä–∞–∑–¥–∞—á–∏ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞ —Å –ü–ö —á–µ—Ä–µ–∑ Ethernet-–∫–∞–±–µ–ª—å –Ω–∞ Smart TV –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ:\n",
      "\n",
      "–û—Å–Ω–æ–≤–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ: –ø–æ–¥–∫–ª—é—á–∏—Ç—å Ethernet-–∫–∞–±–µ–ª—å –æ—Ç –∫–æ–º–ø—å—é—Ç–µ—Ä–∞ –∫ Smart TV –∏ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å —Å–µ—Ç–µ–≤–æ–π —à–ª—é–∑ –∏–ª–∏ DHCP-—Å–µ—Ä–≤–µ—Ä –Ω–∞ –ü–ö –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –¥–æ—Å—Ç—É–ø–∞ –∫ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç—É –Ω–∞ —Ç–µ–ª–µ–≤–∏–∑–æ—Ä–µ.\n",
      "\n",
      "–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏: —É–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ Ethernet-–∫–∞–±–µ–ª—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ –ø–æ–¥–∫–ª—é—á–µ–Ω –∫ –æ–±–æ–∏–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞–º (–ü–ö –∏ Smart TV), –∞ —Ç–∞–∫–∂–µ —á—Ç–æ —Å–µ—Ç–µ–≤–∞—è –∫–∞—Ä—Ç–∞ –Ω–∞ –ü–ö –∏ Smart TV –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –ø—Ä–æ—Ç–æ–∫–æ–ª—ã –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, DHCP –∏ IPv4 –∏–ª–∏ IPv6).\n",
      "\n",
      "–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã: –µ—Å–ª–∏ —É –≤–∞—Å –¥–æ—Å—Ç—É–ø–µ–Ω WiFi –Ω–∞ –ü–ö –∏ Smart TV, –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–µ—Å–ø—Ä–æ–≤–æ–¥–Ω–æ–π —Å–ø–æ—Å–æ–± –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç—É –∏ SKIP Ethernet-–∫–∞–±–µ–ª—å. –í —ç—Ç–æ–º —Å–ª—É—á–∞–µ –Ω–∞—Å—Ç—Ä–æ–π—Ç–µ WiFi-—Å–µ—Ç—å –Ω–∞ –ü–ö –∏ Smart TV —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ. –ï—Å–ª–∏ —É –≤–∞—Å –Ω–µ—Ç –¥–æ—Å—Ç—É–ø–∞ –∫ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º Smart TV –∏–ª–∏ –≤–æ–∑–Ω–∏–∫–Ω—É—Ç –ø—Ä–æ–±–ª–µ–º—ã —Å Ethernet-—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ–º, –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ —Å–µ—Ç–µ–≤–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ,such as a wireless router or a network switch, –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –¥–æ—Å—Ç—É–ø–∞ –∫ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç—É –æ—Ç –ü–ö –∫ Smart TV. –û–¥–Ω–∞–∫–æ —ç—Ç–æ –º–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫ –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π.\n"
     ]
    }
   ],
   "source": [
    "print(get_answer(\"–ö–∞–∫ —Ä–∞–∑–¥–∞—Ç—å –∏–Ω—Ç–µ—Ä–Ω–µ—Ç —Å –ü–ö —á–µ—Ä–µ–∑ Ethernet-–∫–∞–±–µ–ª—å –Ω–∞ Smart TV?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e5d858bc-5252-4040-b093-82b11aa3ee98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   18819.44 ms\n",
      "llama_print_timings:      sample time =      25.33 ms /    65 runs   (    0.39 ms per token,  2566.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7834.99 ms /   141 tokens (   55.57 ms per token,    18.00 tokens per second)\n",
      "llama_print_timings:        eval time =   10178.93 ms /    64 runs   (  159.05 ms per token,     6.29 tokens per second)\n",
      "llama_print_timings:       total time =   18540.40 ms /   205 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ü—Ä–∏ —Å–±—Ä–æ—Å–µ –Ω–∞—Å—Ç—Ä–æ–µ–∫ —Ä–∞–∑–≥–æ–Ω–∞ –ø–æ—Å–ª–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è BIOS –≤–æ–∑–º–æ–∂–Ω–æ–π –ø—Ä–∏—á–∏–Ω–æ–π —è–≤–ª—è–µ—Ç—Å—è –∏–∑–º–µ–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —É—Ç–∏–ª–∏—Ç–∞–º–∏ —Ñ–∏—Ä–º—ã-–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—è –∏–ª–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º —Å–±—Ä–æ—Å–æ–º –∑–∞–≤–æ–¥—Å–∫–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –±–∏–æ—Å–∞ –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã —Å–∏—Å—Ç–µ–º—ã.\n"
     ]
    }
   ],
   "source": [
    "print(get_answer(\"–ü–æ—á–µ–º—É –ø–æ—Å–ª–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è BIOS —Å–±—Ä–æ—Å–∏–ª–∏—Å—å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Ä–∞–∑–≥–æ–Ω–∞?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "18dc0a83-f19e-418e-864b-3c58e3aca4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   18819.44 ms\n",
      "llama_print_timings:      sample time =      18.98 ms /    48 runs   (    0.40 ms per token,  2529.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14657.76 ms /   268 tokens (   54.69 ms per token,    18.28 tokens per second)\n",
      "llama_print_timings:        eval time =    7429.70 ms /    47 runs   (  158.08 ms per token,     6.33 tokens per second)\n",
      "llama_print_timings:       total time =   22476.44 ms /   315 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–î–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∏—è –≤—Ç–æ—Ä–æ–≥–æ –º–æ–Ω–∏—Ç–æ—Ä–∞ –ø—Ä–∏ —Å–≤–æ—Ä–∞—á–∏–≤–∞–Ω–∏–∏ –∏–≥—Ä —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é ¬´–ü–æ—Å—Ç–æ—è–Ω–Ω–æ–µ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —ç–∫—Ä–∞–Ω–∞¬ª –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö –≤–∏–¥–µ–æ–∫–∞—Ä—Ç—ã –∏–ª–∏ –º–æ–Ω–∏—Ç–æ—Ä–∞.\n"
     ]
    }
   ],
   "source": [
    "print(get_answer(\"–ö–∞–∫ —Å–¥–µ–ª–∞—Ç—å —Ç–∞–∫, —á—Ç–æ–±—ã –≤—Ç–æ—Ä–æ–π –º–æ–Ω–∏—Ç–æ—Ä –Ω–µ –æ—Ç–∫–ª—é—á–∞–ª—Å—è –ø—Ä–∏ —Å–≤–æ—Ä–∞—á–∏–≤–∞–Ω–∏–∏ –∏–≥—Ä?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "98c148d2-65c5-40c3-a205-2c5bf41cb16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   19182.00 ms\n",
      "llama_print_timings:      sample time =      89.75 ms /   229 runs   (    0.39 ms per token,  2551.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =   19181.71 ms /   355 tokens (   54.03 ms per token,    18.51 tokens per second)\n",
      "llama_print_timings:        eval time =   36316.63 ms /   228 runs   (  159.28 ms per token,     6.28 tokens per second)\n",
      "llama_print_timings:       total time =   57532.46 ms /   583 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ü—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ –∑–∞—Ä—è–¥–∫–∏ –Ω–æ—É—Ç–±—É–∫–∞ –æ—Ç Thunderbolt-–¥–æ–∫-—Å—Ç–∞–Ω—Ü–∏–∏, –Ω–æ –ø—Ä–∏ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç–µ –¥—Ä—É–≥–∏—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤, –≤–µ—Ä–æ—è—Ç–Ω–æ, –ø—Ä–æ–±–ª–µ–º–∞ —Å–≤—è–∑–∞–Ω–∞ —Å –Ω–µ–∏—Å–ø—Ä–∞–≤–Ω–æ—Å—Ç—å—é –∑–∞—Ä—è–¥–Ω–æ–≥–æ –º–æ–¥—É–ª—è –∏–ª–∏ –Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å—é —Å—Ç–∞–Ω—Ü–∏–∏ —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º —Ç–∏–ø–æ–º –Ω–æ—É—Ç–±—É–∫–∞ –∏–ª–∏ –µ–≥–æ –∑–∞—Ä—è–¥–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã.\n",
      "\n",
      "–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Å—Ç–∞–Ω—Ü–∏–∏ –∏ –Ω–æ—É—Ç–±—É–∫–∞ –ø–æ —Å–ª–µ–¥—É—é—â–∏–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º:\n",
      "* –¢–∏–ø –∑–∞—Ä—è–¥–∫–∏ (USB-C Power Delivery –∏–ª–∏ Thunderbolt Power Delivery);\n",
      "* –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –º–æ—â–Ω–æ—Å—Ç—å –∑–∞—Ä—è–¥–∞;\n",
      "* –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –ø—Ä–æ—Ç–æ–∫–æ–ª—ã –∑–∞—Ä—è–¥–∫–∏.\n",
      "\n",
      "–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç: –í–æ–∑–º–æ–∂–Ω–æ, –ø—Ä–æ–±–ª–µ–º–∞ –≤—ã–∑–≤–∞–Ω–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π —Å—Ç–∞–Ω—Ü–∏–∏ –∏–ª–∏ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–º —Å –¥—Ä—É–≥–∏–º–∏ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞–º–∏, –ø–æ–¥–∫–ª—é—á–µ–Ω–Ω—ã–º–∏ –∫ –Ω–µ–π. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –æ—Ç–∫–ª—é—á–∏—Ç—å –¥—Ä—É–≥–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –∏ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∑–∞—Ä—è–¥–∫—É –Ω–æ—É—Ç–±—É–∫–∞ –æ—Ç–¥–µ–ª—å–Ω–æ. –ï—Å–ª–∏ –ø—Ä–æ–±–ª–µ–º–∞ persists, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—é —Å—Ç–∞–Ω—Ü–∏–∏ –∏–ª–∏ –∫–æ–Ω—Ç–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å —Å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–±—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏ —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã.\n"
     ]
    }
   ],
   "source": [
    "print(get_answer(\"–ü–æ—á–µ–º—É Thunderbolt-–¥–æ–∫-—Å—Ç–∞–Ω—Ü–∏—è –Ω–µ –∑–∞—Ä—è–∂–∞–µ—Ç –Ω–æ—É—Ç–±—É–∫, –Ω–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ —Ä–∞–±–æ—Ç–∞—é—Ç?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbcda68-34bb-46d2-a975-dfc12a8e5b0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
